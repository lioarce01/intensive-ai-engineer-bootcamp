{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Tokenization Tutorial: BPE, WordPiece, and SentencePiece\n",
    "\n",
    "This tutorial provides hands-on experience with the three most important tokenization algorithms used in modern NLP. We'll use real tokenizers from Hugging Face to understand how they work in practice.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how different tokenization algorithms split text\n",
    "- Learn the vocabulary building process for each algorithm\n",
    "- See how OOV (Out-of-Vocabulary) words are handled\n",
    "- Practice with token IDs, encoding, and decoding\n",
    "- Compare algorithms on real examples\n",
    "\n",
    "## Prerequisites\n",
    "Make sure you have the required packages installed:\n",
    "```bash\n",
    "pip install torch transformers tokenizers datasets matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer, BertTokenizer, T5Tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece, Unigram\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Tokenization Fundamentals\n",
    "\n",
    "Before diving into specific algorithms, let's understand what tokenization is and why it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts for our experiments\n",
    "sample_texts = [\n",
    "    \"Hello, world! This is a tokenization tutorial.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning models need to understand text.\",\n",
    "    \"Tokenization is preprocessing for NLP tasks.\",\n",
    "    \"Subword tokenization handles out-of-vocabulary words effectively.\",\n",
    "    \"GPT-2 uses Byte-Pair Encoding for tokenization.\",\n",
    "    \"BERT uses WordPiece tokenization algorithm.\",\n",
    "    \"T5 uses SentencePiece with unigram language model.\"\n",
    "]\n",
    "\n",
    "# Let's also test with some challenging examples\n",
    "challenging_texts = [\n",
    "    \"COVID-19 pandemic started in 2020.\",\n",
    "    \"The transformer architecture revolutionized NLP.\",\n",
    "    \"Hugging Face transformers library is amazing!\",\n",
    "    \"Pre-training and fine-tuning are key concepts.\",\n",
    "    \"Attention is all you need (2017).\",\n",
    "    \"GPT-3.5-turbo and GPT-4 are powerful models.\"\n",
    "]\n",
    "\n",
    "all_texts = sample_texts + challenging_texts\n",
    "\n",
    "print(\"Sample texts prepared for tokenization experiments:\")\n",
    "for i, text in enumerate(all_texts[:3]):\n",
    "    print(f\"{i+1}. {text}\")\n",
    "print(f\"... and {len(all_texts)-3} more texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Byte-Pair Encoding (BPE) - GPT-2 Style\n",
    "\n",
    "BPE is used by GPT models. It starts with characters and iteratively merges the most frequent pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer (uses BPE)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(\"GPT-2 Tokenizer (BPE) Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size: {len(gpt2_tokenizer)}\")\n",
    "print(f\"Model max length: {gpt2_tokenizer.model_max_length}\")\n",
    "print(f\"Special tokens: {gpt2_tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Demonstrate tokenization on sample text\n",
    "test_text = \"Hello, tokenization! This is a test with out-of-vocabulary words like COVID-19.\"\n",
    "print(f\"\\nOriginal text: '{test_text}'\")\n",
    "\n",
    "# Encode to tokens\n",
    "tokens = gpt2_tokenizer.tokenize(test_text)\n",
    "print(f\"\\nTokens ({len(tokens)}): {tokens}\")\n",
    "\n",
    "# Encode to IDs\n",
    "token_ids = gpt2_tokenizer.encode(test_text)\n",
    "print(f\"\\nToken IDs ({len(token_ids)}): {token_ids}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = gpt2_tokenizer.decode(token_ids)\n",
    "print(f\"\\nDecoded text: '{decoded_text}'\")\n",
    "print(f\"Perfect reconstruction: {test_text == decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive BPE Analysis Function\n",
    "def analyze_bpe_tokenization(text, tokenizer, name=\"BPE\"):\n",
    "    \"\"\"Analyze how BPE tokenizes a given text.\"\"\"\n",
    "    print(f\"\\n{name} Tokenization Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Original: '{text}'\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    \n",
    "    # Show character-level breakdown\n",
    "    print(\"\\nToken → Characters mapping:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Handle special BPE encoding (Ġ represents space)\n",
    "        display_token = token.replace('Ġ', '▁')  # Use visible space character\n",
    "        print(f\"  {i:2d}: '{display_token}' → ID {tokenizer.encode(token, add_special_tokens=False)[0] if tokenizer.encode(token, add_special_tokens=False) else 'UNK'}\")\n",
    "    \n",
    "    return tokens, token_ids\n",
    "\n",
    "# Test BPE on various examples\n",
    "test_cases = [\n",
    "    \"hello world\",\n",
    "    \"tokenization\",\n",
    "    \"COVID-19\",\n",
    "    \"transformer\",\n",
    "    \"out-of-vocabulary\"\n",
    "]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    analyze_bpe_tokenization(test_case, gpt2_tokenizer, \"GPT-2 BPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: WordPiece - BERT Style\n",
    "\n",
    "WordPiece is used by BERT and similar models. It uses a greedy approach to build the longest possible subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer (uses WordPiece)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"BERT Tokenizer (WordPiece) Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size: {len(bert_tokenizer)}\")\n",
    "print(f\"Model max length: {bert_tokenizer.model_max_length}\")\n",
    "print(f\"Special tokens: {bert_tokenizer.special_tokens_map}\")\n",
    "\n",
    "# WordPiece specific analysis\n",
    "def analyze_wordpiece_tokenization(text, tokenizer, name=\"WordPiece\"):\n",
    "    \"\"\"Analyze how WordPiece tokenizes a given text.\"\"\"\n",
    "    print(f\"\\n{name} Tokenization Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Original: '{text}'\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    \n",
    "    # Show WordPiece specific features\n",
    "    print(\"\\nWordPiece features:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith('##'):\n",
    "            feature = \"Continuation subword (##)\"\n",
    "        elif token in ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASK]']:\n",
    "            feature = \"Special token\"\n",
    "        else:\n",
    "            feature = \"Word beginning\"\n",
    "        print(f\"  {i:2d}: '{token}' → {feature}\")\n",
    "    \n",
    "    return tokens, token_ids\n",
    "\n",
    "# Test WordPiece on the same examples\n",
    "for test_case in test_cases:\n",
    "    analyze_wordpiece_tokenization(test_case, bert_tokenizer, \"BERT WordPiece\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: SentencePiece - T5 Style\n",
    "\n",
    "SentencePiece is used by T5, mT5, and many multilingual models. It treats the input as a sequence of Unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 tokenizer (uses SentencePiece)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "print(\"T5 Tokenizer (SentencePiece) Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size: {len(t5_tokenizer)}\")\n",
    "print(f\"Model max length: {t5_tokenizer.model_max_length}\")\n",
    "print(f\"Special tokens: {t5_tokenizer.special_tokens_map}\")\n",
    "\n",
    "# SentencePiece specific analysis\n",
    "def analyze_sentencepiece_tokenization(text, tokenizer, name=\"SentencePiece\"):\n",
    "    \"\"\"Analyze how SentencePiece tokenizes a given text.\"\"\"\n",
    "    print(f\"\\n{name} Tokenization Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Original: '{text}'\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    \n",
    "    # Show SentencePiece specific features\n",
    "    print(\"\\nSentencePiece features:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.startswith('▁'):\n",
    "            feature = \"Word boundary marker (▁)\"\n",
    "        elif token in ['<pad>', '</s>', '<unk>']:\n",
    "            feature = \"Special token\"\n",
    "        else:\n",
    "            feature = \"Subword continuation\"\n",
    "        print(f\"  {i:2d}: '{token}' → {feature}\")\n",
    "    \n",
    "    return tokens, token_ids\n",
    "\n",
    "# Test SentencePiece on the same examples\n",
    "for test_case in test_cases:\n",
    "    analyze_sentencepiece_tokenization(test_case, t5_tokenizer, \"T5 SentencePiece\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparative Analysis\n",
    "\n",
    "Let's compare how different tokenizers handle the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(text):\n",
    "    \"\"\"Compare how different tokenizers handle the same text.\"\"\"\n",
    "    print(f\"\\nComparative Analysis for: '{text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get tokenizations from all three\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "    bert_tokens = bert_tokenizer.tokenize(text)\n",
    "    t5_tokens = t5_tokenizer.tokenize(text)\n",
    "    \n",
    "    # Create comparison table\n",
    "    max_tokens = max(len(gpt2_tokens), len(bert_tokens), len(t5_tokens))\n",
    "    \n",
    "    print(f\"{'Index':<6} {'GPT-2 (BPE)':<20} {'BERT (WordPiece)':<20} {'T5 (SentencePiece)':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i in range(max_tokens):\n",
    "        gpt2_token = gpt2_tokens[i] if i < len(gpt2_tokens) else \"\"\n",
    "        bert_token = bert_tokens[i] if i < len(bert_tokens) else \"\"\n",
    "        t5_token = t5_tokens[i] if i < len(t5_tokens) else \"\"\n",
    "        \n",
    "        # Clean up display\n",
    "        gpt2_display = gpt2_token.replace('Ġ', '▁')\n",
    "        \n",
    "        print(f\"{i:<6} {gpt2_display:<20} {bert_token:<20} {t5_token:<20}\")\n",
    "    \n",
    "    print(f\"\\nToken counts: GPT-2: {len(gpt2_tokens)}, BERT: {len(bert_tokens)}, T5: {len(t5_tokens)}\")\n",
    "    \n",
    "    return {\n",
    "        'gpt2': gpt2_tokens,\n",
    "        'bert': bert_tokens,\n",
    "        't5': t5_tokens\n",
    "    }\n",
    "\n",
    "# Compare on interesting examples\n",
    "comparison_texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"COVID-19 pandemic\",\n",
    "    \"transformer architecture\",\n",
    "    \"out-of-vocabulary words\",\n",
    "    \"GPT-3.5-turbo model\"\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "for text in comparison_texts:\n",
    "    comparison_results[text] = compare_tokenizers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token count comparison\n",
    "def visualize_token_counts(comparison_results):\n",
    "    \"\"\"Create a bar chart comparing token counts across tokenizers.\"\"\"\n",
    "    texts = list(comparison_results.keys())\n",
    "    gpt2_counts = [len(comparison_results[text]['gpt2']) for text in texts]\n",
    "    bert_counts = [len(comparison_results[text]['bert']) for text in texts]\n",
    "    t5_counts = [len(comparison_results[text]['t5']) for text in texts]\n",
    "    \n",
    "    # Create DataFrame for easier plotting\n",
    "    df = pd.DataFrame({\n",
    "        'Text': texts,\n",
    "        'GPT-2 (BPE)': gpt2_counts,\n",
    "        'BERT (WordPiece)': bert_counts,\n",
    "        'T5 (SentencePiece)': t5_counts\n",
    "    })\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    x = range(len(texts))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar([i - width for i in x], gpt2_counts, width, label='GPT-2 (BPE)', alpha=0.8)\n",
    "    plt.bar(x, bert_counts, width, label='BERT (WordPiece)', alpha=0.8)\n",
    "    plt.bar([i + width for i in x], t5_counts, width, label='T5 (SentencePiece)', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Text Examples')\n",
    "    plt.ylabel('Number of Tokens')\n",
    "    plt.title('Token Count Comparison Across Tokenizers')\n",
    "    plt.xticks(x, [text[:20] + '...' if len(text) > 20 else text for text in texts], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "token_count_df = visualize_token_counts(comparison_results)\n",
    "print(\"\\nToken Count Summary:\")\n",
    "print(token_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Handling Out-of-Vocabulary (OOV) Words\n",
    "\n",
    "Let's see how each tokenizer handles words not in their vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with completely made-up words and technical terms\n",
    "oov_test_cases = [\n",
    "    \"supercalifragilisticexpialidocious\",\n",
    "    \"antidisestablishmentarianism\",\n",
    "    \"pseudopseudohypoparathyroidism\",\n",
    "    \"Huggingface\",  # Company name\n",
    "    \"tokenizer123\",  # Alphanumeric\n",
    "    \"COVID-19\",  # Hyphenated with numbers\n",
    "    \"@username\",  # Social media handle\n",
    "    \"#hashtag\",  # Hashtag\n",
    "    \"www.example.com\",  # URL\n",
    "    \"user@email.com\"  # Email\n",
    "]\n",
    "\n",
    "def test_oov_handling(text, tokenizer, tokenizer_name):\n",
    "    \"\"\"Test how a tokenizer handles OOV words.\"\"\"\n",
    "    print(f\"\\n{tokenizer_name} OOV Analysis: '{text}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    \n",
    "    # Check for UNK tokens\n",
    "    unk_count = 0\n",
    "    if tokenizer_name == \"BERT\":\n",
    "        unk_count = tokens.count('[UNK]')\n",
    "    elif tokenizer_name == \"T5\":\n",
    "        unk_count = tokens.count('<unk>')\n",
    "    # GPT-2 rarely uses UNK due to BPE falling back to characters\n",
    "    \n",
    "    print(f\"UNK tokens: {unk_count}\")\n",
    "    \n",
    "    # Decode and check for information loss\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    if hasattr(tokenizer, 'clean_up_tokenization'):\n",
    "        decoded = tokenizer.clean_up_tokenization(decoded)\n",
    "    \n",
    "    perfect_reconstruction = (text.lower().strip() == decoded.lower().strip())\n",
    "    print(f\"Decoded: '{decoded}'\")\n",
    "    print(f\"Perfect reconstruction: {perfect_reconstruction}\")\n",
    "    \n",
    "    return tokens, perfect_reconstruction\n",
    "\n",
    "print(\"Testing Out-of-Vocabulary Handling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "oov_results = {}\n",
    "for test_case in oov_test_cases[:5]:  # Test first 5 cases\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: '{test_case}'\")\n",
    "    \n",
    "    gpt2_tokens, gpt2_perfect = test_oov_handling(test_case, gpt2_tokenizer, \"GPT-2\")\n",
    "    bert_tokens, bert_perfect = test_oov_handling(test_case, bert_tokenizer, \"BERT\")\n",
    "    t5_tokens, t5_perfect = test_oov_handling(test_case, t5_tokenizer, \"T5\")\n",
    "    \n",
    "    oov_results[test_case] = {\n",
    "        'gpt2_tokens': len(gpt2_tokens),\n",
    "        'bert_tokens': len(bert_tokens),\n",
    "        't5_tokens': len(t5_tokens),\n",
    "        'gpt2_perfect': gpt2_perfect,\n",
    "        'bert_perfect': bert_perfect,\n",
    "        't5_perfect': t5_perfect\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Building a Simple BPE Tokenizer from Scratch\n",
    "\n",
    "Let's implement a simplified version of BPE to understand the algorithm better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"A simplified implementation of Byte-Pair Encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_freqs = {}\n",
    "        self.vocab = set()\n",
    "        self.merges = []\n",
    "    \n",
    "    def get_word_frequencies(self, texts):\n",
    "        \"\"\"Count word frequencies in the corpus.\"\"\"\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            # Simple word splitting\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                # Remove punctuation for simplicity\n",
    "                word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "                if word:\n",
    "                    word_freq[word] += 1\n",
    "        return word_freq\n",
    "    \n",
    "    def get_initial_vocab(self, word_freqs):\n",
    "        \"\"\"Get initial vocabulary (all characters).\"\"\"\n",
    "        vocab = set()\n",
    "        for word in word_freqs:\n",
    "            for char in word:\n",
    "                vocab.add(char)\n",
    "        return vocab\n",
    "    \n",
    "    def get_pairs(self, word_freqs):\n",
    "        \"\"\"Get all adjacent pairs in the corpus with their frequencies.\"\"\"\n",
    "        pairs = Counter()\n",
    "        for word, freq in word_freqs.items():\n",
    "            chars = list(word)\n",
    "            for i in range(len(chars) - 1):\n",
    "                pairs[(chars[i], chars[i + 1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, word_freqs):\n",
    "        \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
    "        new_word_freqs = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            # Add spaces between characters for replacement\n",
    "            spaced_word = ' '.join(word)\n",
    "            new_word = spaced_word.replace(bigram, replacement)\n",
    "            # Remove spaces\n",
    "            new_word = new_word.replace(' ', '')\n",
    "            new_word_freqs[new_word] = freq\n",
    "        \n",
    "        return new_word_freqs\n",
    "    \n",
    "    def train(self, texts, num_merges=10):\n",
    "        \"\"\"Train the BPE tokenizer.\"\"\"\n",
    "        # Get word frequencies\n",
    "        self.word_freqs = self.get_word_frequencies(texts)\n",
    "        print(f\"Initial vocabulary size: {len(self.get_initial_vocab(self.word_freqs))}\")\n",
    "        print(f\"Most frequent words: {dict(self.word_freqs.most_common(5))}\")\n",
    "        \n",
    "        # Initialize vocabulary with characters\n",
    "        self.vocab = self.get_initial_vocab(self.word_freqs)\n",
    "        \n",
    "        # Perform merges\n",
    "        current_word_freqs = self.word_freqs.copy()\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_pairs(current_word_freqs)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Get most frequent pair\n",
    "            best_pair = pairs.most_common(1)[0][0]\n",
    "            print(f\"\\nMerge {i+1}: {best_pair} (frequency: {pairs[best_pair]})\")\n",
    "            \n",
    "            # Merge the pair\n",
    "            current_word_freqs = self.merge_vocab(best_pair, current_word_freqs)\n",
    "            \n",
    "            # Add merged token to vocabulary\n",
    "            new_token = ''.join(best_pair)\n",
    "            self.vocab.add(new_token)\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            print(f\"New token: '{new_token}'\")\n",
    "            print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        print(f\"\\nFinal vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Final vocabulary: {sorted(self.vocab)}\")\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        \"\"\"Tokenize a word using learned merges.\"\"\"\n",
    "        # Start with characters\n",
    "        tokens = list(word.lower())\n",
    "        \n",
    "        # Apply merges in order\n",
    "        for pair in self.merges:\n",
    "            i = 0\n",
    "            while i < len(tokens) - 1:\n",
    "                if tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "                    # Merge the pair\n",
    "                    tokens = tokens[:i] + [''.join(pair)] + tokens[i + 2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "# Train our simple BPE\n",
    "simple_bpe = SimpleBPE()\n",
    "training_texts = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"machine learning is transforming the world\",\n",
    "    \"tokenization is an important preprocessing step\",\n",
    "    \"the transformer architecture uses attention mechanisms\",\n",
    "    \"natural language processing helps machines understand text\"\n",
    "]\n",
    "\n",
    "print(\"Training Simple BPE Tokenizer\")\n",
    "print(\"=\" * 50)\n",
    "simple_bpe.train(training_texts, num_merges=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our simple BPE tokenizer\n",
    "test_words = [\"the\", \"quick\", \"transformer\", \"tokenization\", \"machine\", \"learning\"]\n",
    "\n",
    "print(\"\\nTesting Simple BPE Tokenizer\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for word in test_words:\n",
    "    tokens = simple_bpe.tokenize(word)\n",
    "    print(f\"'{word}' → {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Interactive Experiments\n",
    "\n",
    "Now let's create some interactive functions to experiment with tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_tokenizer_comparison(text):\n",
    "    \"\"\"Interactive function to compare tokenizers on any text.\"\"\"\n",
    "    print(f\"\\n🔍 Analyzing: '{text}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    tokenizers = {\n",
    "        \"GPT-2 (BPE)\": gpt2_tokenizer,\n",
    "        \"BERT (WordPiece)\": bert_tokenizer,\n",
    "        \"T5 (SentencePiece)\": t5_tokenizer\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        decoded = tokenizer.decode(token_ids)\n",
    "        \n",
    "        results[name] = {\n",
    "            'tokens': tokens,\n",
    "            'count': len(tokens),\n",
    "            'ids': token_ids,\n",
    "            'decoded': decoded,\n",
    "            'perfect': text.strip().lower() == decoded.strip().lower()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Tokens ({len(tokens)}): {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
    "        print(f\"  Token IDs: {token_ids[:10]}{'...' if len(token_ids) > 10 else ''}\")\n",
    "        print(f\"  Perfect reconstruction: {results[name]['perfect']}\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"  Token counts: \", end=\"\")\n",
    "    for name in tokenizers.keys():\n",
    "        print(f\"{name.split()[0]}: {results[name]['count']}\", end=\"  \")\n",
    "    print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with various examples\n",
    "test_examples = [\n",
    "    \"Hello, world!\",\n",
    "    \"The transformer model revolutionized NLP.\",\n",
    "    \"COVID-19 pandemic affected everyone globally.\",\n",
    "    \"GPT-3.5-turbo and GPT-4 are amazing models!\"\n",
    "]\n",
    "\n",
    "for example in test_examples:\n",
    "    interactive_tokenizer_comparison(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to explore vocabulary\n",
    "def explore_vocabulary(tokenizer, name, sample_size=20):\n",
    "    \"\"\"Explore the vocabulary of a tokenizer.\"\"\"\n",
    "    print(f\"\\n🔤 {name} Vocabulary Exploration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    vocab_size = len(tokenizer)\n",
    "    print(f\"Total vocabulary size: {vocab_size:,}\")\n",
    "    \n",
    "    # Get some sample tokens\n",
    "    if hasattr(tokenizer, 'get_vocab'):\n",
    "        vocab = tokenizer.get_vocab()\n",
    "        \n",
    "        # Show some interesting tokens\n",
    "        print(f\"\\nSample tokens (first {sample_size}):\")\n",
    "        sample_tokens = list(vocab.keys())[:sample_size]\n",
    "        for i, token in enumerate(sample_tokens):\n",
    "            token_id = vocab[token]\n",
    "            # Clean up display\n",
    "            display_token = token.replace('Ġ', '▁').replace('##', '##')\n",
    "            print(f\"  {i:2d}: '{display_token}' (ID: {token_id})\")\n",
    "        \n",
    "        # Look for special patterns\n",
    "        print(f\"\\nSpecial patterns found:\")\n",
    "        \n",
    "        if name == \"GPT-2 (BPE)\":\n",
    "            space_tokens = [token for token in vocab.keys() if token.startswith('Ġ')][:5]\n",
    "            print(f\"  Space-prefixed tokens: {[t.replace('Ġ', '▁') for t in space_tokens]}\")\n",
    "        \n",
    "        elif name == \"BERT (WordPiece)\":\n",
    "            subword_tokens = [token for token in vocab.keys() if token.startswith('##')][:5]\n",
    "            print(f\"  Subword tokens (##): {subword_tokens}\")\n",
    "            special_tokens = [token for token in vocab.keys() if token.startswith('[') and token.endswith(']')]\n",
    "            print(f\"  Special tokens: {special_tokens}\")\n",
    "        \n",
    "        elif name == \"T5 (SentencePiece)\":\n",
    "            boundary_tokens = [token for token in vocab.keys() if token.startswith('▁')][:5]\n",
    "            print(f\"  Boundary tokens (▁): {boundary_tokens}\")\n",
    "            special_tokens = [token for token in vocab.keys() if token.startswith('<') and token.endswith('>')]\n",
    "            print(f\"  Special tokens: {special_tokens}\")\n",
    "\n# Explore vocabularies\n",
    "explore_vocabulary(gpt2_tokenizer, \"GPT-2 (BPE)\")\n",
    "explore_vocabulary(bert_tokenizer, \"BERT (WordPiece)\")\n",
    "explore_vocabulary(t5_tokenizer, \"T5 (SentencePiece)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Practical Applications and Best Practices\n",
    "\n",
    "Let's explore some practical considerations when working with tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate sequence length considerations\n",
    "def analyze_sequence_lengths(texts, tokenizers_dict):\n",
    "    \"\"\"Analyze how sequence lengths vary across tokenizers.\"\"\"\n",
    "    print(\"📏 Sequence Length Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers_dict.items():\n",
    "        lengths = []\n",
    "        for text in texts:\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "            lengths.append(len(tokens))\n",
    "        \n",
    "        results[name] = {\n",
    "            'lengths': lengths,\n",
    "            'mean': sum(lengths) / len(lengths),\n",
    "            'max': max(lengths),\n",
    "            'min': min(lengths)\n",
    "        }\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"{'Tokenizer':<20} {'Mean':<8} {'Min':<6} {'Max':<6}\")\n",
    "    print(\"-\" * 45)\n",
    "    for name, stats in results.items():\n",
    "        print(f\"{name:<20} {stats['mean']:<8.1f} {stats['min']:<6} {stats['max']:<6}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with our sample texts\n",
    "tokenizers_dict = {\n",
    "    \"GPT-2 (BPE)\": gpt2_tokenizer,\n",
    "    \"BERT (WordPiece)\": bert_tokenizer,\n",
    "    \"T5 (SentencePiece)\": t5_tokenizer\n",
    "}\n",
    "\n",
    "length_results = analyze_sequence_lengths(all_texts, tokenizers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate batch processing and padding\n",
    "def demonstrate_batch_processing():\n",
    "    \"\"\"Show how to handle batches of texts with different lengths.\"\"\"\n",
    "    print(\"🔄 Batch Processing Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample batch with different lengths\n",
    "    batch_texts = [\n",
    "        \"Short text.\",\n",
    "        \"This is a medium-length text for demonstration.\",\n",
    "        \"This is a much longer text that will result in more tokens when tokenized by our various tokenizers, which is useful for demonstrating padding and truncation strategies.\"\n",
    "    ]\n",
    "    \n",
    "    # Use BERT tokenizer for demonstration\n",
    "    tokenizer = bert_tokenizer\n",
    "    \n",
    "    print(\"Original texts:\")\n",
    "    for i, text in enumerate(batch_texts):\n",
    "        print(f\"  {i+1}: {text[:50]}{'...' if len(text) > 50 else ''}\")\n",
    "    \n",
    "    # Tokenize without padding\n",
    "    print(\"\\nTokenized (no padding):\")\n",
    "    for i, text in enumerate(batch_texts):\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        print(f\"  {i+1}: Length {len(tokens)}, IDs: {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
    "    \n",
    "    # Batch encode with padding\n",
    "    batch_encoding = tokenizer(\n",
    "        batch_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=50,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBatch encoded (with padding):\")\n",
    "    print(f\"  Input IDs shape: {batch_encoding['input_ids'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch_encoding['attention_mask'].shape}\")\n",
    "    \n",
    "    print(\"\\nPadded sequences:\")\n",
    "    for i in range(len(batch_texts)):\n",
    "        input_ids = batch_encoding['input_ids'][i].tolist()\n",
    "        attention_mask = batch_encoding['attention_mask'][i].tolist()\n",
    "        \n",
    "        # Count actual tokens (non-padded)\n",
    "        actual_tokens = sum(attention_mask)\n",
    "        print(f\"  {i+1}: Actual tokens: {actual_tokens}, Total length: {len(input_ids)}\")\n",
    "        print(f\"      Input IDs: {input_ids[:15]}{'...' if len(input_ids) > 15 else ''}\")\n",
    "        print(f\"      Attention : {attention_mask[:15]}{'...' if len(attention_mask) > 15 else ''}\")\n",
    "\n",
    "demonstrate_batch_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Key Takeaways and Best Practices\n",
    "\n",
    "Let's summarize what we've learned about tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenization_summary():\n",
    "    \"\"\"Create a comprehensive summary of tokenization concepts.\"\"\"\n",
    "    \n",
    "    summary = \"\"\"\n",
    "    🎯 TOKENIZATION SUMMARY & BEST PRACTICES\n",
    "    ======================================\n",
    "    \n",
    "    📚 ALGORITHM COMPARISON:\n",
    "    \n",
    "    BPE (Byte-Pair Encoding) - GPT-2 Style:\n",
    "    ✅ Excellent OOV handling (falls back to characters)\n",
    "    ✅ Deterministic and reversible\n",
    "    ✅ Good for generative tasks\n",
    "    ⚠️  Can create very long sequences for rare words\n",
    "    \n",
    "    WordPiece - BERT Style:\n",
    "    ✅ Balances vocabulary size and sequence length\n",
    "    ✅ Good for understanding tasks\n",
    "    ✅ Handles morphology reasonably well\n",
    "    ⚠️  Uses [UNK] for truly unknown words (information loss)\n",
    "    \n",
    "    SentencePiece - T5 Style:\n",
    "    ✅ Language-agnostic (handles any Unicode)\n",
    "    ✅ No need for pre-tokenization\n",
    "    ✅ Great for multilingual models\n",
    "    ✅ Reversible tokenization\n",
    "    ⚠️  Requires larger vocabulary for same coverage\n",
    "    \n",
    "    🛠️ ENGINEERING BEST PRACTICES:\n",
    "    \n",
    "    1. Always use the same tokenizer for training and inference\n",
    "    2. Pay attention to special tokens ([CLS], [SEP], <s>, </s>, etc.)\n",
    "    3. Handle padding and truncation appropriately for your task\n",
    "    4. Consider sequence length limits of your model\n",
    "    5. Test tokenization on edge cases (URLs, emails, code, etc.)\n",
    "    6. Use batch processing for efficiency\n",
    "    7. Understand the trade-offs between vocab size and sequence length\n",
    "    \n",
    "    💡 PRACTICAL TIPS:\n",
    "    \n",
    "    • BPE: Best for generation, handles any text\n",
    "    • WordPiece: Good balance for most tasks\n",
    "    • SentencePiece: Essential for multilingual applications\n",
    "    \n",
    "    • Smaller vocab = longer sequences\n",
    "    • Larger vocab = shorter sequences but more parameters\n",
    "    \n",
    "    • Always validate your tokenization pipeline\n",
    "    • Monitor for unexpected tokenizations\n",
    "    • Consider domain-specific vocabularies for specialized text\n",
    "    \"\"\"\n",
    "    \n",
    "    print(summary)\n",
    "\ncreate_tokenization_summary()\n",
    "\n",
    "# Final interactive test function\n",
    "def final_tokenization_test(user_text):\n",
    "    \"\"\"Final test function for user input.\"\"\"\n",
    "    print(f\"\\n🧪 Final Test with Your Text: '{user_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return interactive_tokenizer_comparison(user_text)\n",
    "\n",
    "# Test with a final example\n",
    "test_result = final_tokenization_test(\"The future of AI and NLP is bright with transformers!\")\n",
    "\n",
    "print(\"\\n🎉 Congratulations! You've completed the tokenization tutorial!\")\n",
    "print(\"Now you understand how BPE, WordPiece, and SentencePiece work in practice.\")\n",
    "print(\"\\n💡 Next steps:\")\n",
    "print(\"• Experiment with different texts using the functions above\")\n",
    "print(\"• Try training your own tokenizer on domain-specific data\")\n",
    "print(\"• Explore how tokenization affects model performance\")\n",
    "print(\"• Learn about other tokenization strategies (Unigram, etc.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}