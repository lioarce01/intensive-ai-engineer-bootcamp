{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Tutorial: From Theory to Practice\n",
    "\n",
    "This comprehensive tutorial covers word embeddings fundamentals with hands-on PyTorch implementation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Word Embeddings](#introduction)\n",
    "2. [Traditional vs Modern Approaches](#traditional-vs-modern)\n",
    "3. [Word2Vec Implementation from Scratch](#word2vec-implementation)\n",
    "4. [Working with Pre-trained GloVe Embeddings](#glove-embeddings)\n",
    "5. [Interactive Demonstrations](#demonstrations)\n",
    "6. [Training Custom Embeddings](#custom-training)\n",
    "7. [Real-world Applications](#applications)\n",
    "8. [Semantic Search Foundation](#semantic-search)\n",
    "\n",
    "## Requirements\n",
    "- Python 3.13+\n",
    "- PyTorch 2.8+\n",
    "- NumPy, Matplotlib, Scikit-learn\n",
    "- NLTK for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Word Embeddings {#introduction}\n",
    "\n",
    "Word embeddings are dense vector representations of words that capture semantic relationships. Unlike one-hot encoding, embeddings:\n",
    "\n",
    "- **Capture semantic similarity**: Similar words have similar vectors\n",
    "- **Enable vector arithmetic**: Mathematical operations reveal relationships\n",
    "- **Reduce dimensionality**: Dense representations (50-300D) vs sparse one-hot\n",
    "- **Transfer learning**: Pre-trained embeddings work across tasks\n",
    "\n",
    "### Key Concepts\n",
    "- **Distributional Hypothesis**: Words appearing in similar contexts have similar meanings\n",
    "- **Context Window**: Surrounding words that define meaning\n",
    "- **Embedding Space**: High-dimensional space where semantic relationships are preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demonstration of the distributional hypothesis\n",
    "sample_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A dog ran in the park\", \n",
    "    \"The kitten played on the carpet\",\n",
    "    \"A puppy walked in the garden\",\n",
    "    \"The feline rested on the rug\"\n",
    "]\n",
    "\n",
    "def extract_context_words(sentences, target_word, window_size=2):\n",
    "    \"\"\"Extract context words for a target word from sentences.\"\"\"\n",
    "    contexts = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word == target_word:\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(words), i + window_size + 1)\n",
    "                context = words[start:i] + words[i+1:end]\n",
    "                contexts.extend(context)\n",
    "    \n",
    "    return Counter(contexts)\n",
    "\n",
    "# Compare context words for semantically similar words\n",
    "cat_contexts = extract_context_words(sample_sentences, \"cat\")\n",
    "dog_contexts = extract_context_words(sample_sentences, \"dog\")\n",
    "kitten_contexts = extract_context_words(sample_sentences, \"kitten\")\n",
    "\n",
    "print(\"Context words for 'cat':\", dict(cat_contexts))\n",
    "print(\"Context words for 'dog':\", dict(dog_contexts))\n",
    "print(\"Context words for 'kitten':\", dict(kitten_contexts))\n",
    "\n",
    "print(\"\\nNotice how 'cat' and 'kitten' share more context words than 'cat' and 'dog'\")\n",
    "print(\"This is the foundation of word embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional vs Modern Approaches {#traditional-vs-modern}\n",
    "\n",
    "### Traditional Approaches\n",
    "- **One-hot encoding**: Sparse, no semantic information\n",
    "- **Co-occurrence matrices**: Dense but computationally expensive\n",
    "- **TF-IDF**: Good for document similarity, poor for semantic similarity\n",
    "\n",
    "### Modern Approaches\n",
    "- **Word2Vec**: Efficient neural approach (Skip-gram, CBOW)\n",
    "- **GloVe**: Global statistical information + local context\n",
    "- **FastText**: Subword information for out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of traditional vs modern approaches\n",
    "vocab = [\"cat\", \"dog\", \"kitten\", \"puppy\", \"car\", \"truck\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# One-hot encoding example\n",
    "def create_one_hot(word, vocab):\n",
    "    \"\"\"Create one-hot encoding for a word.\"\"\"\n",
    "    vector = np.zeros(len(vocab))\n",
    "    if word in vocab:\n",
    "        vector[vocab.index(word)] = 1\n",
    "    return vector\n",
    "\n",
    "# Create one-hot vectors\n",
    "cat_onehot = create_one_hot(\"cat\", vocab)\n",
    "dog_onehot = create_one_hot(\"dog\", vocab)\n",
    "kitten_onehot = create_one_hot(\"kitten\", vocab)\n",
    "\n",
    "print(\"One-hot vectors (sparse, no semantic information):\")\n",
    "print(f\"cat:    {cat_onehot}\")\n",
    "print(f\"dog:    {dog_onehot}\")\n",
    "print(f\"kitten: {kitten_onehot}\")\n",
    "\n",
    "# Calculate similarities\n",
    "cat_dog_sim = np.dot(cat_onehot, dog_onehot)\n",
    "cat_kitten_sim = np.dot(cat_onehot, kitten_onehot)\n",
    "\n",
    "print(f\"\\nOne-hot similarities:\")\n",
    "print(f\"cat-dog similarity: {cat_dog_sim}\")\n",
    "print(f\"cat-kitten similarity: {cat_kitten_sim}\")\n",
    "print(\"Problem: All word pairs have zero similarity!\")\n",
    "\n",
    "# Mock dense embeddings (what we'll learn to create)\n",
    "print(\"\\nDense embeddings (what we'll create):\")\n",
    "mock_embeddings = {\n",
    "    \"cat\":    np.array([0.2, -0.1, 0.8, -0.3]),\n",
    "    \"dog\":    np.array([0.1, -0.2, 0.7, -0.1]),\n",
    "    \"kitten\": np.array([0.3, -0.1, 0.9, -0.4]),\n",
    "    \"car\":    np.array([-0.5, 0.8, 0.1, 0.2])\n",
    "}\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "cat_dog_sim_dense = cosine_sim(mock_embeddings[\"cat\"], mock_embeddings[\"dog\"])\n",
    "cat_kitten_sim_dense = cosine_sim(mock_embeddings[\"cat\"], mock_embeddings[\"kitten\"])\n",
    "cat_car_sim_dense = cosine_sim(mock_embeddings[\"cat\"], mock_embeddings[\"car\"])\n",
    "\n",
    "print(f\"cat-dog similarity: {cat_dog_sim_dense:.3f}\")\n",
    "print(f\"cat-kitten similarity: {cat_kitten_sim_dense:.3f}\")\n",
    "print(f\"cat-car similarity: {cat_car_sim_dense:.3f}\")\n",
    "print(\"Success: Similar words (cat-kitten) have higher similarity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec Implementation from Scratch {#word2vec-implementation}\n",
    "\n",
    "Word2Vec uses shallow neural networks to learn word embeddings. Two architectures:\n",
    "\n",
    "### Skip-gram\n",
    "- Input: Center word\n",
    "- Output: Context words\n",
    "- Better for rare words\n",
    "\n",
    "### CBOW (Continuous Bag of Words)\n",
    "- Input: Context words\n",
    "- Output: Center word\n",
    "- Faster training, better for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Preprocessor for text data to prepare for Word2Vec training.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_count=5):\n",
    "        self.min_count = min_count\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.word_counts = Counter()\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove non-alphabetic characters except spaces\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def build_vocab(self, corpus: List[str]) -> None:\n",
    "        \"\"\"Build vocabulary from corpus.\"\"\"\n",
    "        # Count all words\n",
    "        for sentence in corpus:\n",
    "            cleaned = self.clean_text(sentence)\n",
    "            words = cleaned.split()\n",
    "            self.word_counts.update(words)\n",
    "        \n",
    "        # Filter by minimum count\n",
    "        filtered_words = [word for word, count in self.word_counts.items() \n",
    "                         if count >= self.min_count]\n",
    "        \n",
    "        # Create mappings\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(filtered_words)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "        \n",
    "        print(f\"Built vocabulary with {self.vocab_size} words\")\n",
    "        print(f\"Most common words: {self.word_counts.most_common(10)}\")\n",
    "    \n",
    "    def encode_sentence(self, sentence: str) -> List[int]:\n",
    "        \"\"\"Convert sentence to list of word indices.\"\"\"\n",
    "        cleaned = self.clean_text(sentence)\n",
    "        words = cleaned.split()\n",
    "        return [self.word_to_idx[word] for word in words if word in self.word_to_idx]\n",
    "\n",
    "# Sample corpus for demonstration\n",
    "sample_corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A dog ran in the park\",\n",
    "    \"The cat played with the dog\", \n",
    "    \"Dogs and cats are pets\",\n",
    "    \"The park has many dogs running\",\n",
    "    \"Cats like to sit on mats\",\n",
    "    \"The dog and cat are friends\",\n",
    "    \"Animals play in the park daily\",\n",
    "    \"The mat is comfortable for cats\",\n",
    "    \"Dogs love to run and play\"\n",
    "]\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(min_count=1)  # Low min_count for small corpus\n",
    "preprocessor.build_vocab(sample_corpus)\n",
    "\n",
    "# Show encoded sentences\n",
    "print(\"\\nEncoded sentences:\")\n",
    "for i, sentence in enumerate(sample_corpus[:3]):\n",
    "    encoded = preprocessor.encode_sentence(sentence)\n",
    "    decoded = [preprocessor.idx_to_word[idx] for idx in encoded]\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Encoded:  {encoded}\")\n",
    "    print(f\"Decoded:  {decoded}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    \"\"\"Dataset for Word2Vec training (Skip-gram model).\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[str], preprocessor: TextPreprocessor, \n",
    "                 window_size: int = 2):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.window_size = window_size\n",
    "        self.pairs = self._create_training_pairs(corpus)\n",
    "    \n",
    "    def _create_training_pairs(self, corpus: List[str]) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Create (center_word, context_word) pairs for Skip-gram.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            encoded = self.preprocessor.encode_sentence(sentence)\n",
    "            \n",
    "            for i, center_word in enumerate(encoded):\n",
    "                # Define context window\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(encoded), i + self.window_size + 1)\n",
    "                \n",
    "                # Create pairs with context words\n",
    "                for j in range(start, end):\n",
    "                    if i != j:  # Skip center word itself\n",
    "                        context_word = encoded[j]\n",
    "                        pairs.append((center_word, context_word))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "# Create dataset\n",
    "dataset = Word2VecDataset(sample_corpus, preprocessor, window_size=2)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Created {len(dataset)} training pairs\")\n",
    "print(\"Sample pairs:\")\n",
    "for i in range(5):\n",
    "    center_idx, context_idx = dataset.pairs[i]\n",
    "    center_word = preprocessor.idx_to_word[center_idx]\n",
    "    context_word = preprocessor.idx_to_word[context_idx]\n",
    "    print(f\"  {center_word} -> {context_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"Skip-gram Word2Vec model implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Input embeddings (center words)\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Output embeddings (context words)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self._init_embeddings()\n",
    "    \n",
    "    def _init_embeddings(self):\n",
    "        \"\"\"Initialize embeddings with small random values.\"\"\"\n",
    "        nn.init.uniform_(self.in_embeddings.weight, -0.5/self.embedding_dim, 0.5/self.embedding_dim)\n",
    "        nn.init.uniform_(self.out_embeddings.weight, -0.5/self.embedding_dim, 0.5/self.embedding_dim)\n",
    "    \n",
    "    def forward(self, center_words: torch.Tensor, context_words: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for Skip-gram model.\"\"\"\n",
    "        # Get embeddings\n",
    "        center_embeds = self.in_embeddings(center_words)  # (batch_size, embedding_dim)\n",
    "        context_embeds = self.out_embeddings(context_words)  # (batch_size, embedding_dim)\n",
    "        \n",
    "        # Compute dot product (similarity)\n",
    "        scores = torch.sum(center_embeds * context_embeds, dim=1)  # (batch_size,)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_word_embedding(self, word_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Get embedding for a specific word.\"\"\"\n",
    "        return self.in_embeddings.weight[word_idx].detach()\n",
    "    \n",
    "    def get_all_embeddings(self) -> torch.Tensor:\n",
    "        \"\"\"Get all word embeddings.\"\"\"\n",
    "        return self.in_embeddings.weight.detach()\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 50\n",
    "model = SkipGramModel(preprocessor.vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"Initialized Skip-gram model:\")\n",
    "print(f\"  Vocabulary size: {preprocessor.vocab_size}\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(model, dataloader, optimizer, num_epochs=100, negative_samples=5):\n",
    "    \"\"\"Train Word2Vec model with negative sampling.\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    print(\"Training Word2Vec model...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (center_words, context_words) in enumerate(dataloader):\n",
    "            center_words = center_words.to(device)\n",
    "            context_words = context_words.to(device)\n",
    "            batch_size = center_words.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Positive samples (actual context words)\n",
    "            pos_scores = model(center_words, context_words)\n",
    "            pos_loss = -F.logsigmoid(pos_scores).mean()\n",
    "            \n",
    "            # Negative samples (random words)\n",
    "            neg_words = torch.randint(0, model.vocab_size, (batch_size * negative_samples,), device=device)\n",
    "            center_repeated = center_words.repeat_interleave(negative_samples)\n",
    "            neg_scores = model(center_repeated, neg_words)\n",
    "            neg_loss = -F.logsigmoid(-neg_scores).mean()\n",
    "            \n",
    "            # Total loss\n",
    "            loss = pos_loss + neg_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return losses\n",
    "\n",
    "# Train the model\n",
    "losses = train_word2vec(model, dataloader, optimizer, num_epochs=200)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Word2Vec Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Pre-trained GloVe Embeddings {#glove-embeddings}\n",
    "\n",
    "GloVe (Global Vectors) combines global statistical information with local context information. Let's load and work with pre-trained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeEmbeddings:\n",
    "    \"\"\"Loader and utilities for GloVe embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.embeddings = None\n",
    "        self.embedding_dim = 0\n",
    "    \n",
    "    def download_glove(self, dim=50):\n",
    "        \"\"\"Download GloVe embeddings if not present.\"\"\"\n",
    "        filename = f\"glove.6B.{dim}d.txt\"\n",
    "        filepath = os.path.join(\".\", filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading GloVe {dim}d embeddings...\")\n",
    "            url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "            \n",
    "            # Download and extract\n",
    "            urllib.request.urlretrieve(url, \"glove.6B.zip\")\n",
    "            with zipfile.ZipFile(\"glove.6B.zip\", 'r') as zip_ref:\n",
    "                zip_ref.extract(filename)\n",
    "            os.remove(\"glove.6B.zip\")\n",
    "            print(\"Download completed!\")\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    def load_glove_subset(self, words_to_load=None, dim=50):\n",
    "        \"\"\"Load a subset of GloVe embeddings for demonstration.\"\"\"\n",
    "        # For demo purposes, create mock GloVe embeddings\n",
    "        demo_words = [\n",
    "            \"cat\", \"dog\", \"kitten\", \"puppy\", \"animal\", \"pet\",\n",
    "            \"king\", \"queen\", \"man\", \"woman\", \"royal\", \"person\",\n",
    "            \"car\", \"truck\", \"vehicle\", \"drive\", \"road\", \"traffic\",\n",
    "            \"happy\", \"sad\", \"joy\", \"anger\", \"emotion\", \"feeling\",\n",
    "            \"big\", \"small\", \"large\", \"tiny\", \"size\", \"scale\",\n",
    "            \"good\", \"bad\", \"excellent\", \"terrible\", \"quality\"\n",
    "        ]\n",
    "        \n",
    "        if words_to_load:\n",
    "            demo_words.extend(words_to_load)\n",
    "        \n",
    "        # Create realistic embeddings with semantic relationships\n",
    "        np.random.seed(42)\n",
    "        self.embedding_dim = dim\n",
    "        embeddings_dict = {}\n",
    "        \n",
    "        # Define semantic clusters\n",
    "        clusters = {\n",
    "            'animals': [\"cat\", \"dog\", \"kitten\", \"puppy\", \"animal\", \"pet\"],\n",
    "            'royalty': [\"king\", \"queen\", \"man\", \"woman\", \"royal\", \"person\"],\n",
    "            'vehicles': [\"car\", \"truck\", \"vehicle\", \"drive\", \"road\", \"traffic\"],\n",
    "            'emotions': [\"happy\", \"sad\", \"joy\", \"anger\", \"emotion\", \"feeling\"],\n",
    "            'sizes': [\"big\", \"small\", \"large\", \"tiny\", \"size\", \"scale\"],\n",
    "            'quality': [\"good\", \"bad\", \"excellent\", \"terrible\", \"quality\"]\n",
    "        }\n",
    "        \n",
    "        # Generate embeddings with cluster structure\n",
    "        for cluster_name, words in clusters.items():\n",
    "            # Create cluster center\n",
    "            cluster_center = np.random.randn(dim) * 0.5\n",
    "            \n",
    "            for word in words:\n",
    "                if word in demo_words:\n",
    "                    # Add noise around cluster center\n",
    "                    embedding = cluster_center + np.random.randn(dim) * 0.3\n",
    "                    embeddings_dict[word] = embedding\n",
    "        \n",
    "        # Handle remaining words\n",
    "        for word in demo_words:\n",
    "            if word not in embeddings_dict:\n",
    "                embeddings_dict[word] = np.random.randn(dim) * 0.5\n",
    "        \n",
    "        # Create mappings and embedding matrix\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(embeddings_dict.keys())}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        \n",
    "        embedding_list = [embeddings_dict[word] for word in self.word_to_idx.keys()]\n",
    "        self.embeddings = torch.tensor(embedding_list, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Loaded {len(self.word_to_idx)} GloVe embeddings ({dim}D)\")\n",
    "        return self.embeddings\n",
    "    \n",
    "    def get_embedding(self, word: str) -> Optional[torch.Tensor]:\n",
    "        \"\"\"Get embedding for a word.\"\"\"\n",
    "        if word in self.word_to_idx:\n",
    "            idx = self.word_to_idx[word]\n",
    "            return self.embeddings[idx]\n",
    "        return None\n",
    "    \n",
    "    def similarity(self, word1: str, word2: str) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two words.\"\"\"\n",
    "        emb1 = self.get_embedding(word1)\n",
    "        emb2 = self.get_embedding(word2)\n",
    "        \n",
    "        if emb1 is None or emb2 is None:\n",
    "            return 0.0\n",
    "        \n",
    "        cos_sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0))\n",
    "        return cos_sim.item()\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVeEmbeddings()\n",
    "glove_embeddings = glove.load_glove_subset(dim=100)\n",
    "\n",
    "# Test similarities\n",
    "print(\"\\nGloVe Similarity Examples:\")\n",
    "test_pairs = [\n",
    "    (\"cat\", \"dog\"),\n",
    "    (\"cat\", \"kitten\"),\n",
    "    (\"king\", \"queen\"),\n",
    "    (\"car\", \"truck\"),\n",
    "    (\"happy\", \"sad\"),\n",
    "    (\"big\", \"large\"),\n",
    "    (\"cat\", \"car\")\n",
    "]\n",
    "\n",
    "for word1, word2 in test_pairs:\n",
    "    sim = glove.similarity(word1, word2)\n",
    "    print(f\"{word1} - {word2}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Demonstrations {#demonstrations}\n",
    "\n",
    "Let's explore the fascinating properties of word embeddings through interactive demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAnalyzer:\n",
    "    \"\"\"Analyzer for exploring word embedding properties.\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, word_to_idx, idx_to_word):\n",
    "        self.embeddings = embeddings\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "    \n",
    "    def get_embedding(self, word: str) -> Optional[torch.Tensor]:\n",
    "        \"\"\"Get embedding for a word.\"\"\"\n",
    "        if word in self.word_to_idx:\n",
    "            idx = self.word_to_idx[word]\n",
    "            return self.embeddings[idx]\n",
    "        return None\n",
    "    \n",
    "    def find_nearest_neighbors(self, word: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find k nearest neighbors to a word.\"\"\"\n",
    "        word_emb = self.get_embedding(word)\n",
    "        if word_emb is None:\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities to all words\n",
    "        similarities = F.cosine_similarity(word_emb.unsqueeze(0), self.embeddings)\n",
    "        \n",
    "        # Get top k similar words (excluding the word itself)\n",
    "        top_k = torch.topk(similarities, k + 1)\n",
    "        \n",
    "        neighbors = []\n",
    "        for score, idx in zip(top_k.values[1:], top_k.indices[1:]):  # Skip first (itself)\n",
    "            neighbor_word = self.idx_to_word[idx.item()]\n",
    "            neighbors.append((neighbor_word, score.item()))\n",
    "        \n",
    "        return neighbors\n",
    "    \n",
    "    def vector_arithmetic(self, positive: List[str], negative: List[str] = None, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Perform vector arithmetic: positive[0] + positive[1] + ... - negative[0] - negative[1] - ...\"\"\"\n",
    "        if negative is None:\n",
    "            negative = []\n",
    "        \n",
    "        result_vector = torch.zeros_like(self.embeddings[0])\n",
    "        \n",
    "        # Add positive vectors\n",
    "        for word in positive:\n",
    "            emb = self.get_embedding(word)\n",
    "            if emb is not None:\n",
    "                result_vector += emb\n",
    "        \n",
    "        # Subtract negative vectors\n",
    "        for word in negative:\n",
    "            emb = self.get_embedding(word)\n",
    "            if emb is not None:\n",
    "                result_vector -= emb\n",
    "        \n",
    "        # Find nearest neighbors to result vector\n",
    "        similarities = F.cosine_similarity(result_vector.unsqueeze(0), self.embeddings)\n",
    "        \n",
    "        # Get top k words, excluding input words\n",
    "        excluded_words = set(positive + negative)\n",
    "        top_words = []\n",
    "        \n",
    "        sorted_indices = torch.argsort(similarities, descending=True)\n",
    "        for idx in sorted_indices:\n",
    "            word = self.idx_to_word[idx.item()]\n",
    "            if word not in excluded_words:\n",
    "                score = similarities[idx].item()\n",
    "                top_words.append((word, score))\n",
    "                if len(top_words) >= k:\n",
    "                    break\n",
    "        \n",
    "        return top_words\n",
    "    \n",
    "    def solve_analogy(self, a: str, b: str, c: str, k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Solve analogy: a is to b as c is to ?\"\"\"\n",
    "        # a:b :: c:? => ? = b - a + c\n",
    "        return self.vector_arithmetic(positive=[b, c], negative=[a], k=k)\n",
    "\n",
    "# Test with our trained Word2Vec embeddings\n",
    "w2v_analyzer = EmbeddingAnalyzer(\n",
    "    model.get_all_embeddings(),\n",
    "    preprocessor.word_to_idx,\n",
    "    preprocessor.idx_to_word\n",
    ")\n",
    "\n",
    "print(\"=== Word2Vec Embeddings Analysis ===\")\n",
    "print(\"\\nNearest neighbors:\")\n",
    "test_words = [\"cat\", \"dog\", \"park\"]\n",
    "for word in test_words:\n",
    "    if word in preprocessor.word_to_idx:\n",
    "        neighbors = w2v_analyzer.find_nearest_neighbors(word, k=3)\n",
    "        print(f\"{word}: {[(w, f'{s:.3f}') for w, s in neighbors]}\")\n",
    "\n",
    "# Test with GloVe embeddings\n",
    "glove_analyzer = EmbeddingAnalyzer(\n",
    "    glove.embeddings,\n",
    "    glove.word_to_idx,\n",
    "    glove.idx_to_word\n",
    ")\n",
    "\n",
    "print(\"\\n=== GloVe Embeddings Analysis ===\")\n",
    "print(\"\\nNearest neighbors:\")\n",
    "for word in [\"cat\", \"king\", \"happy\"]:\n",
    "    neighbors = glove_analyzer.find_nearest_neighbors(word, k=3)\n",
    "    print(f\"{word}: {[(w, f'{s:.3f}') for w, s in neighbors]}\")\n",
    "\n",
    "print(\"\\nVector arithmetic examples:\")\n",
    "# King - man + woman = queen\n",
    "result = glove_analyzer.solve_analogy(\"man\", \"king\", \"woman\", k=3)\n",
    "print(f\"man:king :: woman:? = {[(w, f'{s:.3f}') for w, s in result]}\")\n",
    "\n",
    "# Cat + big = ?\n",
    "result = glove_analyzer.vector_arithmetic([\"cat\", \"big\"], k=3)\n",
    "print(f\"cat + big = {[(w, f'{s:.3f}') for w, s in result]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, word_to_idx, idx_to_word, method='tsne', words_to_plot=None):\n",
    "    \"\"\"Visualize word embeddings using dimensionality reduction.\"\"\"\n",
    "    \n",
    "    # Select words to plot\n",
    "    if words_to_plot is None:\n",
    "        # Select a subset of words\n",
    "        words_to_plot = list(word_to_idx.keys())[:30]\n",
    "    \n",
    "    # Get embeddings for selected words\n",
    "    indices = [word_to_idx[word] for word in words_to_plot if word in word_to_idx]\n",
    "    selected_embeddings = embeddings[indices].numpy()\n",
    "    selected_words = [idx_to_word[idx] for idx in indices]\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(selected_words)-1))\n",
    "        coords = reducer.fit_transform(selected_embeddings)\n",
    "        title = 't-SNE Visualization of Word Embeddings'\n",
    "    elif method == 'pca':\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        coords = reducer.fit_transform(selected_embeddings)\n",
    "        title = f'PCA Visualization of Word Embeddings\\n(Explained variance: {reducer.explained_variance_ratio_.sum():.2%})'\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'tsne' or 'pca'\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(coords[:, 0], coords[:, 1], alpha=0.7, s=100)\n",
    "    \n",
    "    # Add word labels\n",
    "    for i, word in enumerate(selected_words):\n",
    "        plt.annotate(word, (coords[i, 0], coords[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    ha='left', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(f'{method.upper()} Component 1')\n",
    "    plt.ylabel(f'{method.upper()} Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize GloVe embeddings\n",
    "print(\"Visualizing GloVe embeddings...\")\n",
    "visualize_embeddings(\n",
    "    glove.embeddings, \n",
    "    glove.word_to_idx, \n",
    "    glove.idx_to_word, \n",
    "    method='pca'\n",
    ")\n",
    "\n",
    "visualize_embeddings(\n",
    "    glove.embeddings, \n",
    "    glove.word_to_idx, \n",
    "    glove.idx_to_word, \n",
    "    method='tsne'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_heatmap(analyzer, words):\n",
    "    \"\"\"Create a heatmap showing similarity between words.\"\"\"\n",
    "    n = len(words)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            if word1 in analyzer.word_to_idx and word2 in analyzer.word_to_idx:\n",
    "                emb1 = analyzer.get_embedding(word1)\n",
    "                emb2 = analyzer.get_embedding(word2)\n",
    "                sim = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "                similarity_matrix[i, j] = sim\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=words, \n",
    "                yticklabels=words, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                fmt='.2f')\n",
    "    plt.title('Word Similarity Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create similarity heatmap for selected words\n",
    "selected_words = [\"cat\", \"dog\", \"kitten\", \"puppy\", \"king\", \"queen\", \"man\", \"woman\", \"car\", \"truck\"]\n",
    "available_words = [w for w in selected_words if w in glove_analyzer.word_to_idx]\n",
    "\n",
    "print(f\"Creating similarity heatmap for words: {available_words}\")\n",
    "create_similarity_heatmap(glove_analyzer, available_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Custom Embeddings {#custom-training}\n",
    "\n",
    "Let's create a more sophisticated training setup for custom embeddings with a larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_corpus(size=1000):\n",
    "    \"\"\"Generate a larger sample corpus for training.\"\"\"\n",
    "    templates = [\n",
    "        \"The {animal} {verb} in the {location}\",\n",
    "        \"A {size} {animal} {verb} {adverb}\",\n",
    "        \"The {color} {object} is {adjective}\",\n",
    "        \"{person} {verb} the {object} {adverb}\",\n",
    "        \"In the {location}, {animal}s {verb} {adverb}\",\n",
    "        \"The {adjective} {object} {verb} {location}\",\n",
    "        \"{person} saw a {size} {color} {animal}\",\n",
    "        \"Every {animal} {verb} when {condition}\",\n",
    "        \"The {location} has many {color} {object}s\",\n",
    "        \"{size} {animal}s are {adjective} and {adjective}\"\n",
    "    ]\n",
    "    \n",
    "    words = {\n",
    "        'animal': ['cat', 'dog', 'bird', 'fish', 'rabbit', 'mouse', 'lion', 'tiger', 'elephant', 'bear'],\n",
    "        'verb': ['runs', 'jumps', 'sleeps', 'eats', 'plays', 'walks', 'sits', 'stands', 'flies', 'swims'],\n",
    "        'location': ['park', 'house', 'garden', 'forest', 'river', 'mountain', 'field', 'street', 'beach', 'cave'],\n",
    "        'size': ['big', 'small', 'large', 'tiny', 'huge', 'little', 'giant', 'miniature'],\n",
    "        'color': ['red', 'blue', 'green', 'yellow', 'black', 'white', 'brown', 'orange', 'purple', 'gray'],\n",
    "        'object': ['car', 'house', 'tree', 'flower', 'book', 'chair', 'table', 'window', 'door', 'ball'],\n",
    "        'adjective': ['beautiful', 'ugly', 'fast', 'slow', 'happy', 'sad', 'bright', 'dark', 'clean', 'dirty'],\n",
    "        'person': ['John', 'Mary', 'Bob', 'Alice', 'Tom', 'Sarah', 'Mike', 'Lisa', 'David', 'Emma'],\n",
    "        'adverb': ['quickly', 'slowly', 'quietly', 'loudly', 'carefully', 'happily', 'sadly', 'gently'],\n",
    "        'condition': ['hungry', 'tired', 'excited', 'scared', 'curious', 'bored', 'surprised']\n",
    "    }\n",
    "    \n",
    "    corpus = []\n",
    "    for _ in range(size):\n",
    "        template = random.choice(templates)\n",
    "        sentence = template\n",
    "        \n",
    "        for category, word_list in words.items():\n",
    "            if f'{{{category}}}' in sentence:\n",
    "                word = random.choice(word_list)\n",
    "                sentence = sentence.replace(f'{{{category}}}', word, 1)\n",
    "        \n",
    "        corpus.append(sentence)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "# Generate larger corpus\n",
    "large_corpus = generate_sample_corpus(2000)\n",
    "print(f\"Generated corpus with {len(large_corpus)} sentences\")\n",
    "print(\"\\nSample sentences:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {large_corpus[i]}\")\n",
    "\n",
    "# Train new embeddings on larger corpus\n",
    "large_preprocessor = TextPreprocessor(min_count=3)\n",
    "large_preprocessor.build_vocab(large_corpus)\n",
    "\n",
    "large_dataset = Word2VecDataset(large_corpus, large_preprocessor, window_size=3)\n",
    "large_dataloader = DataLoader(large_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize larger model\n",
    "large_model = SkipGramModel(large_preprocessor.vocab_size, embedding_dim=100).to(device)\n",
    "large_optimizer = optim.Adam(large_model.parameters(), lr=0.005)\n",
    "\n",
    "print(f\"\\nTraining model on {len(large_dataset)} word pairs\")\n",
    "print(f\"Vocabulary size: {large_preprocessor.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the larger model\n",
    "large_losses = train_word2vec(large_model, large_dataloader, large_optimizer, num_epochs=150)\n",
    "\n",
    "# Analyze the trained embeddings\n",
    "large_analyzer = EmbeddingAnalyzer(\n",
    "    large_model.get_all_embeddings(),\n",
    "    large_preprocessor.word_to_idx,\n",
    "    large_preprocessor.idx_to_word\n",
    ")\n",
    "\n",
    "print(\"\\n=== Analysis of Custom Trained Embeddings ===\")\n",
    "print(\"\\nNearest neighbors:\")\n",
    "test_words = [\"cat\", \"dog\", \"big\", \"small\", \"red\", \"blue\"]\n",
    "for word in test_words:\n",
    "    if word in large_preprocessor.word_to_idx:\n",
    "        neighbors = large_analyzer.find_nearest_neighbors(word, k=4)\n",
    "        print(f\"{word}: {[(w, f'{s:.3f}') for w, s in neighbors]}\")\n",
    "\n",
    "print(\"\\nVector arithmetic:\")\n",
    "# Test semantic relationships\n",
    "if all(w in large_preprocessor.word_to_idx for w in [\"big\", \"small\", \"cat\"]):\n",
    "    result = large_analyzer.vector_arithmetic([\"cat\", \"big\"], [\"small\"], k=3)\n",
    "    print(f\"cat + big - small = {[(w, f'{s:.3f}') for w, s in result]}\")\n",
    "\n",
    "if all(w in large_preprocessor.word_to_idx for w in [\"lion\", \"cat\", \"dog\"]):\n",
    "    result = large_analyzer.solve_analogy(\"cat\", \"lion\", \"dog\", k=3)\n",
    "    print(f\"cat:lion :: dog:? = {[(w, f'{s:.3f}') for w, s in result]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world Applications {#applications}\n",
    "\n",
    "Let's explore practical applications of word embeddings in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSimilarity:\n",
    "    \"\"\"Document similarity using word embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer):\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def document_embedding(self, document: str, method='mean') -> torch.Tensor:\n",
    "        \"\"\"Create document embedding by aggregating word embeddings.\"\"\"\n",
    "        words = document.lower().split()\n",
    "        embeddings = []\n",
    "        \n",
    "        for word in words:\n",
    "            emb = self.analyzer.get_embedding(word)\n",
    "            if emb is not None:\n",
    "                embeddings.append(emb)\n",
    "        \n",
    "        if not embeddings:\n",
    "            return torch.zeros(self.analyzer.embeddings.size(1))\n",
    "        \n",
    "        embeddings = torch.stack(embeddings)\n",
    "        \n",
    "        if method == 'mean':\n",
    "            return embeddings.mean(dim=0)\n",
    "        elif method == 'sum':\n",
    "            return embeddings.sum(dim=0)\n",
    "        elif method == 'max':\n",
    "            return embeddings.max(dim=0)[0]\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'mean', 'sum', or 'max'\")\n",
    "    \n",
    "    def similarity(self, doc1: str, doc2: str) -> float:\n",
    "        \"\"\"Calculate similarity between two documents.\"\"\"\n",
    "        emb1 = self.document_embedding(doc1)\n",
    "        emb2 = self.document_embedding(doc2)\n",
    "        \n",
    "        return F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "    \n",
    "    def find_similar_documents(self, query: str, documents: List[str], k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find k most similar documents to a query.\"\"\"\n",
    "        query_emb = self.document_embedding(query)\n",
    "        similarities = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_emb = self.document_embedding(doc)\n",
    "            sim = F.cosine_similarity(query_emb.unsqueeze(0), doc_emb.unsqueeze(0)).item()\n",
    "            similarities.append((doc, sim))\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:k]\n",
    "\n",
    "# Test document similarity\n",
    "doc_sim = DocumentSimilarity(glove_analyzer)\n",
    "\n",
    "sample_documents = [\n",
    "    \"The cat sat on the mat and slept peacefully\",\n",
    "    \"A dog ran quickly through the green park\",\n",
    "    \"The king ruled his kingdom with great wisdom\",\n",
    "    \"A small kitten played with a ball of yarn\",\n",
    "    \"The queen wore a beautiful golden crown\",\n",
    "    \"Cars and trucks drive on the busy highway\",\n",
    "    \"Happy children played games in the sunny park\",\n",
    "    \"The royal family lived in a magnificent palace\"\n",
    "]\n",
    "\n",
    "query = \"cute animals playing\"\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nMost similar documents:\")\n",
    "\n",
    "similar_docs = doc_sim.find_similar_documents(query, sample_documents, k=5)\n",
    "for i, (doc, sim) in enumerate(similar_docs, 1):\n",
    "    print(f\"{i}. {doc} (similarity: {sim:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordClustering:\n",
    "    \"\"\"Cluster words based on their embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer):\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def cluster_words(self, words: List[str], n_clusters: int = 3) -> Dict[int, List[str]]:\n",
    "        \"\"\"Cluster words using K-means.\"\"\"\n",
    "        # Get embeddings for words\n",
    "        embeddings = []\n",
    "        valid_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            emb = self.analyzer.get_embedding(word)\n",
    "            if emb is not None:\n",
    "                embeddings.append(emb.numpy())\n",
    "                valid_words.append(word)\n",
    "        \n",
    "        if len(embeddings) < n_clusters:\n",
    "            return {0: valid_words}\n",
    "        \n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Group words by cluster\n",
    "        clusters = defaultdict(list)\n",
    "        for word, label in zip(valid_words, cluster_labels):\n",
    "            clusters[label].append(word)\n",
    "        \n",
    "        return dict(clusters)\n",
    "\n",
    "# Test word clustering\n",
    "clustering = WordClustering(glove_analyzer)\n",
    "\n",
    "words_to_cluster = [\n",
    "    \"cat\", \"dog\", \"kitten\", \"puppy\", \"animal\", \"pet\",  # Animals\n",
    "    \"king\", \"queen\", \"royal\", \"man\", \"woman\", \"person\",  # People/Royalty\n",
    "    \"car\", \"truck\", \"vehicle\", \"road\", \"drive\",  # Vehicles\n",
    "    \"happy\", \"sad\", \"joy\", \"anger\", \"emotion\", \"feeling\",  # Emotions\n",
    "    \"big\", \"small\", \"large\", \"tiny\", \"size\"\n",
    "]\n",
    "\n",
    "print(\"Clustering words based on embeddings:\")\n",
    "clusters = clustering.cluster_words(words_to_cluster, n_clusters=5)\n",
    "\n",
    "for cluster_id, words in clusters.items():\n",
    "    print(f\"\\nCluster {cluster_id}: {words}\")\n",
    "    \n",
    "    # Show cluster centroid's nearest neighbors\n",
    "    if len(words) > 1:\n",
    "        # Calculate cluster centroid\n",
    "        cluster_embeddings = [glove_analyzer.get_embedding(w) for w in words]\n",
    "        cluster_embeddings = [e for e in cluster_embeddings if e is not None]\n",
    "        if cluster_embeddings:\n",
    "            centroid = torch.stack(cluster_embeddings).mean(dim=0)\n",
    "            \n",
    "            # Find words closest to centroid\n",
    "            similarities = F.cosine_similarity(centroid.unsqueeze(0), glove_analyzer.embeddings)\n",
    "            top_indices = torch.topk(similarities, 5).indices\n",
    "            centroid_words = [glove_analyzer.idx_to_word[idx.item()] for idx in top_indices]\n",
    "            print(f\"  Centroid neighbors: {centroid_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Semantic Search Foundation {#semantic-search}\n",
    "\n",
    "Let's build the foundation for semantic search using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"Simple semantic search engine using word embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer):\n",
    "        self.analyzer = analyzer\n",
    "        self.documents = []\n",
    "        self.document_embeddings = []\n",
    "    \n",
    "    def add_document(self, doc_id: str, content: str):\n",
    "        \"\"\"Add a document to the search index.\"\"\"\n",
    "        self.documents.append({'id': doc_id, 'content': content})\n",
    "        \n",
    "        # Create document embedding\n",
    "        doc_emb = self._create_document_embedding(content)\n",
    "        self.document_embeddings.append(doc_emb)\n",
    "    \n",
    "    def _create_document_embedding(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Create embedding for a document.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        embeddings = []\n",
    "        \n",
    "        for word in words:\n",
    "            emb = self.analyzer.get_embedding(word)\n",
    "            if emb is not None:\n",
    "                embeddings.append(emb)\n",
    "        \n",
    "        if not embeddings:\n",
    "            return torch.zeros(self.analyzer.embeddings.size(1))\n",
    "        \n",
    "        return torch.stack(embeddings).mean(dim=0)\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for documents similar to the query.\"\"\"\n",
    "        query_emb = self._create_document_embedding(query)\n",
    "        \n",
    "        if not self.document_embeddings:\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities\n",
    "        doc_emb_tensor = torch.stack(self.document_embeddings)\n",
    "        similarities = F.cosine_similarity(query_emb.unsqueeze(0), doc_emb_tensor)\n",
    "        \n",
    "        # Get top k results\n",
    "        top_k = torch.topk(similarities, min(k, len(self.documents)))\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(top_k.values, top_k.indices):\n",
    "            doc = self.documents[idx.item()]\n",
    "            results.append({\n",
    "                'id': doc['id'],\n",
    "                'content': doc['content'],\n",
    "                'score': score.item()\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def explain_search(self, query: str, doc_content: str) -> Dict:\n",
    "        \"\"\"Explain why a document matches a query.\"\"\"\n",
    "        query_words = query.lower().split()\n",
    "        doc_words = doc_content.lower().split()\n",
    "        \n",
    "        # Find word-level similarities\n",
    "        word_similarities = []\n",
    "        \n",
    "        for q_word in query_words:\n",
    "            q_emb = self.analyzer.get_embedding(q_word)\n",
    "            if q_emb is None:\n",
    "                continue\n",
    "                \n",
    "            best_match = None\n",
    "            best_score = -1\n",
    "            \n",
    "            for d_word in doc_words:\n",
    "                d_emb = self.analyzer.get_embedding(d_word)\n",
    "                if d_emb is None:\n",
    "                    continue\n",
    "                    \n",
    "                sim = F.cosine_similarity(q_emb.unsqueeze(0), d_emb.unsqueeze(0)).item()\n",
    "                if sim > best_score:\n",
    "                    best_score = sim\n",
    "                    best_match = d_word\n",
    "            \n",
    "            if best_match:\n",
    "                word_similarities.append((q_word, best_match, best_score))\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'document': doc_content,\n",
    "            'word_matches': word_similarities,\n",
    "            'overall_score': self._create_document_embedding(query) @ self._create_document_embedding(doc_content)\n",
    "        }\n",
    "\n",
    "# Create semantic search engine\n",
    "search_engine = SemanticSearchEngine(glove_analyzer)\n",
    "\n",
    "# Add sample documents\n",
    "sample_docs = [\n",
    "    (\"doc1\", \"Cats are wonderful pets that love to play and sleep\"),\n",
    "    (\"doc2\", \"Dogs are loyal animals that enjoy running in parks\"),\n",
    "    (\"doc3\", \"The king and queen ruled their kingdom wisely\"),\n",
    "    (\"doc4\", \"Small kittens are adorable and playful creatures\"),\n",
    "    (\"doc5\", \"Royal families live in magnificent palaces\"),\n",
    "    (\"doc6\", \"Happy children play games in sunny weather\"),\n",
    "    (\"doc7\", \"Cars and trucks transport people and goods\"),\n",
    "    (\"doc8\", \"Emotions like joy and sadness are part of life\")\n",
    "]\n",
    "\n",
    "for doc_id, content in sample_docs:\n",
    "    search_engine.add_document(doc_id, content)\n",
    "\n",
    "print(f\"Added {len(sample_docs)} documents to search engine\")\n",
    "\n",
    "# Test searches\n",
    "test_queries = [\n",
    "    \"cute animals\",\n",
    "    \"royal palace\",\n",
    "    \"happy kids\",\n",
    "    \"vehicle transportation\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n=== Search: '{query}' ===\")\n",
    "    results = search_engine.search(query, k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. {result['content']} (score: {result['score']:.3f})\")\n",
    "    \n",
    "    # Explain top result\n",
    "    if results:\n",
    "        explanation = search_engine.explain_search(query, results[0]['content'])\n",
    "        print(f\"\\nExplanation for top result:\")\n",
    "        for q_word, d_word, score in explanation['word_matches']:\n",
    "            print(f\"  '{q_word}' matches '{d_word}' (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "### What We Learned\n",
    "1. **Word Embedding Fundamentals**: How embeddings capture semantic relationships\n",
    "2. **Word2Vec Implementation**: Skip-gram model from scratch using PyTorch\n",
    "3. **GloVe Embeddings**: Working with pre-trained global vectors\n",
    "4. **Vector Arithmetic**: Mathematical operations revealing word relationships\n",
    "5. **Visualization**: PCA and t-SNE for understanding embedding spaces\n",
    "6. **Real-world Applications**: Document similarity, clustering, and semantic search\n",
    "\n",
    "### Key Insights\n",
    "- **Distributional Hypothesis**: Words in similar contexts have similar meanings\n",
    "- **Dense Representations**: Much more efficient and informative than sparse one-hot\n",
    "- **Transfer Learning**: Pre-trained embeddings work across different tasks\n",
    "- **Semantic Relationships**: Vector arithmetic captures linguistic relationships\n",
    "\n",
    "### Next Steps\n",
    "1. **Modern Transformers**: Move to contextualized embeddings (BERT, GPT)\n",
    "2. **Sentence Embeddings**: Learn sentence-transformers for document-level semantics\n",
    "3. **Multilingual Models**: Explore cross-lingual embeddings\n",
    "4. **Production Systems**: Scale semantic search to large document collections\n",
    "5. **Domain Adaptation**: Fine-tune embeddings for specific domains\n",
    "\n",
    "### Engineering Takeaways\n",
    "- Start with pre-trained embeddings for quick prototyping\n",
    "- Custom training pays off for domain-specific applications\n",
    "- Visualizations are crucial for understanding and debugging\n",
    "- Semantic search enables powerful information retrieval systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: Interactive word exploration\n",
    "def interactive_word_explorer(analyzer, word):\n",
    "    \"\"\"Comprehensive analysis of a word's embedding properties.\"\"\"\n",
    "    if word not in analyzer.word_to_idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== Exploring '{word}' ===\")\n",
    "    \n",
    "    # Nearest neighbors\n",
    "    neighbors = analyzer.find_nearest_neighbors(word, k=5)\n",
    "    print(f\"\\nNearest neighbors:\")\n",
    "    for neighbor, score in neighbors:\n",
    "        print(f\"  {neighbor}: {score:.3f}\")\n",
    "    \n",
    "    # Vector properties\n",
    "    embedding = analyzer.get_embedding(word)\n",
    "    print(f\"\\nEmbedding properties:\")\n",
    "    print(f\"  Dimension: {embedding.size(0)}\")\n",
    "    print(f\"  L2 norm: {torch.norm(embedding).item():.3f}\")\n",
    "    print(f\"  Mean value: {embedding.mean().item():.3f}\")\n",
    "    print(f\"  Std deviation: {embedding.std().item():.3f}\")\n",
    "    \n",
    "    # Test analogies if relevant words exist\n",
    "    analogy_tests = [\n",
    "        ([\"king\", \"man\"], [word], \"royal analogy\"),\n",
    "        ([\"big\", \"small\"], [word], \"size analogy\"),\n",
    "        ([\"happy\", \"sad\"], [word], \"emotion analogy\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nAnalogy tests:\")\n",
    "    for pos_words, neg_words, description in analogy_tests:\n",
    "        if all(w in analyzer.word_to_idx for w in pos_words + neg_words):\n",
    "            result = analyzer.vector_arithmetic(pos_words, neg_words, k=3)\n",
    "            if result:\n",
    "                top_result = result[0]\n",
    "                print(f\"  {description}: {' + '.join(pos_words)} - {' - '.join(neg_words)} = {top_result[0]} ({top_result[1]:.3f})\")\n",
    "\n",
    "# Explore some interesting words\n",
    "words_to_explore = [\"cat\", \"king\", \"happy\"]\n",
    "for word in words_to_explore:\n",
    "    interactive_word_explorer(glove_analyzer, word)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Tutorial completed! You now have a solid foundation in word embeddings.\")\n",
    "print(\"Next: Explore sentence-transformers and modern contextualized embeddings!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}