# 02-Word Embeddings: From Words to Vectors

This section covers word embeddings from classical approaches (Word2Vec, GloVe) to modern techniques (Sentence Transformers).

## 📋 What You'll Learn

- Word2Vec (Skip-gram and CBOW) implementation from scratch
- Working with pre-trained GloVe embeddings
- Vector arithmetic and semantic relationships
- Similarity search and nearest neighbors
- Visualization techniques (PCA, t-SNE)
- Modern sentence-level embeddings

## 📁 Files

- **`word_embeddings_tutorial.ipynb`** - Interactive Jupyter notebook
- **`word_embeddings_comprehensive.py`** - Standalone Python script
- **`WORD_EMBEDDINGS_GUIDE.md`** - Complete documentation
- **`requirements_embeddings.txt`** - Dependencies

## 🚀 Quick Start

```bash
cd 02-word-embeddings
pip install -r requirements_embeddings.txt
python word_embeddings_comprehensive.py
```

## 📚 Key Concepts

- **Word2Vec**: Learning embeddings from context windows
- **GloVe**: Global word-word co-occurrence statistics
- **Vector Arithmetic**: king - man + woman = queen
- **Semantic Search**: Finding similar words/documents

---
**Status**: ✅ Complete - Ready for hands-on learning