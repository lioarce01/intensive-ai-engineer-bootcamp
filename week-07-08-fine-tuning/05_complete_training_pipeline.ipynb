{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Step 5: Complete LoRA Fine-tuning Pipeline\n",
    "\n",
    "## Week 7-8: Production-Ready Training System\n",
    "\n",
    "This is where everything comes together! We'll build a **professional-grade training pipeline** that you could use in real companies.\n",
    "\n",
    "### üéØ What You'll Learn:\n",
    "1. **Professional training loops** - How to structure training for production\n",
    "2. **Monitoring and metrics** - Track progress like a real ML engineer\n",
    "3. **Model evaluation** - Comprehensive performance analysis\n",
    "4. **Comparison baselines** - LoRA vs Full Fine-tuning vs Pre-trained\n",
    "5. **Deployment preparation** - Making models production-ready\n",
    "\n",
    "### üè¢ Why This Matters:\n",
    "This is **exactly** what you'd do as an AI engineer:\n",
    "- Build robust training pipelines\n",
    "- Monitor model performance\n",
    "- Compare different approaches\n",
    "- Prepare models for deployment\n",
    "- Document results for stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import everything we need and set up our environment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üöÄ Complete Training Pipeline Setup\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÖ Training started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üéØ Project: Email Classification with LoRA Fine-tuning\")\n",
    "print(f\"üìö Bootcamp: Week 7-8 - Fine-tuning with LoRA/QLoRA, PEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìß Step 1: Load Data and Model from Previous Step\n",
    "\n",
    "We'll recreate the essential components from Step 4 to ensure this notebook runs independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the email dataset generator from Step 4\n",
    "class EmailDatasetGenerator:\n",
    "    def __init__(self):\n",
    "        self.categories = {\n",
    "            'urgent': 0, 'support': 1, 'sales': 2, 'spam': 3, 'normal': 4\n",
    "        }\n",
    "        \n",
    "        self.email_templates = {\n",
    "            'urgent': [\n",
    "                \"URGENT: System is down and affecting all customers. Please respond immediately.\",\n",
    "                \"CRITICAL: Security breach detected. Need immediate action from the team.\",\n",
    "                \"EMERGENCY: Client meeting in 1 hour, presentation file corrupted. Help needed now!\",\n",
    "                \"URGENT RESPONSE NEEDED: Major bug in production causing revenue loss.\"\n",
    "            ],\n",
    "            'support': [\n",
    "                \"Hi, I'm having trouble logging into my account. Can you help me reset my password?\",\n",
    "                \"Hello, the software keeps crashing when I try to export data. What should I do?\",\n",
    "                \"I can't find the feature you mentioned in the tutorial. Could you guide me?\",\n",
    "                \"The mobile app is not syncing with my desktop. How can I fix this?\"\n",
    "            ],\n",
    "            'sales': [\n",
    "                \"I'm interested in your enterprise plan. Can you send me pricing information?\",\n",
    "                \"We're looking for a solution for our team of 50 people. What do you recommend?\",\n",
    "                \"Could we schedule a demo to see how your product fits our needs?\",\n",
    "                \"I saw your product at the conference. Can we discuss a potential partnership?\"\n",
    "            ],\n",
    "            'spam': [\n",
    "                \"Congratulations! You've won $1,000,000! Click here to claim your prize now!\",\n",
    "                \"AMAZING OFFER: Buy one get one free! Limited time only! Act now!\",\n",
    "                \"Your account will be suspended unless you verify immediately. Click this link.\",\n",
    "                \"Hot singles in your area want to meet you! Join now for free!\"\n",
    "            ],\n",
    "            'normal': [\n",
    "                \"Thanks for the meeting yesterday. Here are the notes we discussed.\",\n",
    "                \"The project timeline looks good. Let's proceed with the next phase.\",\n",
    "                \"I've reviewed the documents and have a few questions. Can we chat tomorrow?\",\n",
    "                \"The team meeting is scheduled for Friday at 2 PM in conference room B.\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def generate_email(self, category: str) -> Dict[str, str]:\n",
    "        body = random.choice(self.email_templates[category])\n",
    "        subject = f\"Re: {body.split('.')[0][:30]}...\"\n",
    "        \n",
    "        return {\n",
    "            'subject': subject,\n",
    "            'body': body,\n",
    "            'category': category,\n",
    "            'label': self.categories[category]\n",
    "        }\n",
    "    \n",
    "    def generate_dataset(self, samples_per_category: int = 150) -> pd.DataFrame:\n",
    "        emails = []\n",
    "        for category in self.categories.keys():\n",
    "            for _ in range(samples_per_category):\n",
    "                email = self.generate_email(category)\n",
    "                emails.append(email)\n",
    "        \n",
    "        random.shuffle(emails)\n",
    "        return pd.DataFrame(emails)\n",
    "\n",
    "# Generate dataset\n",
    "generator = EmailDatasetGenerator()\n",
    "email_df = generator.generate_dataset(samples_per_category=150)\n",
    "\n",
    "print(f\"üìß Generated dataset: {len(email_df)} emails across {len(generator.categories)} categories\")\n",
    "print(f\"   Categories: {list(generator.categories.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "class EmailTextPreprocessor:\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def prepare_for_training(self, df: pd.DataFrame, test_size: float = 0.2) -> Tuple:\n",
    "        # Clean and combine text\n",
    "        df['clean_subject'] = df['subject'].apply(self.clean_text)\n",
    "        df['clean_body'] = df['body'].apply(self.clean_text)\n",
    "        df['combined_text'] = df['clean_subject'] + ' ' + df['clean_body']\n",
    "        \n",
    "        X = df['combined_text'].values\n",
    "        y = df['label'].values\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, df\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = EmailTextPreprocessor()\n",
    "X_train, X_test, y_train, y_test, processed_df = preprocessor.prepare_for_training(email_df)\n",
    "\n",
    "print(f\"üìä Data split: {len(X_train)} train, {len(X_test)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 2: Build LoRA Model and Components\n",
    "\n",
    "Now let's create our LoRA model and all necessary components for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA implementation\n",
    "class AdvancedLoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer: nn.Linear, rank: int = 4, alpha: float = 32.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        self.in_features = original_layer.in_features\n",
    "        self.out_features = original_layer.out_features\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.empty(rank, self.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.empty(self.out_features, rank))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        self.reset_lora_parameters()\n",
    "        \n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def reset_lora_parameters(self):\n",
    "        import math\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.original_layer(x)\n",
    "        lora_output = F.linear(x, self.lora_A)\n",
    "        lora_output = self.dropout(lora_output)\n",
    "        lora_output = F.linear(lora_output, self.lora_B.T)\n",
    "        result += lora_output * self.scaling\n",
    "        return result\n",
    "    \n",
    "    def get_lora_parameters(self):\n",
    "        return [self.lora_A, self.lora_B]\n",
    "\n",
    "# Dataset class\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding='max_length',\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ LoRA components defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Email Classifier\n",
    "class LoRAEmailClassifier(nn.Module):\n",
    "    def __init__(self, model_name: str = 'distilbert-base-uncased', num_classes: int = 5, \n",
    "                 lora_rank: int = 8, lora_alpha: float = 16.0, lora_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"üèóÔ∏è  Building LoRA Email Classifier...\")\n",
    "        print(f\"   Base model: {model_name}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "        print(f\"   LoRA rank: {lora_rank}, alpha: {lora_alpha}\")\n",
    "        \n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Apply LoRA to attention layers\n",
    "        self.lora_layers = {}\n",
    "        self._apply_lora(lora_rank, lora_alpha, lora_dropout)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.backbone.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        self._print_parameter_stats()\n",
    "    \n",
    "    def _apply_lora(self, rank: int, alpha: float, dropout: float):\n",
    "        target_modules = ['query', 'key', 'value']\n",
    "        replaced_count = 0\n",
    "        \n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if any(target in name for target in target_modules) and isinstance(module, nn.Linear):\n",
    "                lora_layer = AdvancedLoRALayer(module, rank=rank, alpha=alpha, dropout=dropout)\n",
    "                \n",
    "                parent_module = self.backbone\n",
    "                module_parts = name.split('.')\n",
    "                \n",
    "                for part in module_parts[:-1]:\n",
    "                    parent_module = getattr(parent_module, part)\n",
    "                \n",
    "                setattr(parent_module, module_parts[-1], lora_layer)\n",
    "                self.lora_layers[name] = lora_layer\n",
    "                replaced_count += 1\n",
    "        \n",
    "        print(f\"   ‚úÖ Applied LoRA to {replaced_count} layers\")\n",
    "    \n",
    "    def _print_parameter_stats(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\nüìä Model Statistics:\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "        print(f\"   Memory reduction: {total_params / trainable_params:.1f}x\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'loss': loss,\n",
    "            'predictions': torch.argmax(logits, dim=-1)\n",
    "        }\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "\n",
    "# Create model\n",
    "model = LoRAEmailClassifier(\n",
    "    model_name='distilbert-base-uncased',\n",
    "    num_classes=5,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16.0\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ LoRA Email Classifier built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Professional Training Pipeline\n",
    "\n",
    "Now let's create our production-ready training system with monitoring, early stopping, and comprehensive logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = EmailDataset(\n",
    "    texts=X_train.tolist(),\n",
    "    labels=y_train.tolist(),\n",
    "    tokenizer=model.tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = EmailDataset(\n",
    "    texts=X_test.tolist(),\n",
    "    labels=y_test.tolist(),\n",
    "    tokenizer=model.tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ Datasets created:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Testing batches: {len(test_loader)}\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_batch = next(iter(train_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=sample_batch['input_ids'],\n",
    "        attention_mask=sample_batch['attention_mask'],\n",
    "        labels=sample_batch['labels']\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Forward pass test successful:\")\n",
    "print(f\"   Output logits shape: {outputs['logits'].shape}\")\n",
    "print(f\"   Loss: {outputs['loss'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional Training Class\n",
    "class LoRAEmailTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, learning_rate: float = 2e-4,\n",
    "                 weight_decay: float = 0.01, max_epochs: int = 5, patience: int = 3, device: str = 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        \n",
    "        self.trainable_params = model.get_trainable_parameters()\n",
    "        self.optimizer = torch.optim.AdamW(self.trainable_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        self.current_epoch = 0\n",
    "        self.best_val_accuracy = 0.0\n",
    "        self.best_model_state = None\n",
    "        self.patience_counter = 0\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_accuracy': [],\n",
    "            'val_loss': [], 'val_accuracy': [],\n",
    "            'learning_rates': [], 'epoch_times': []\n",
    "        }\n",
    "        \n",
    "        print(f\"üîß Trainer initialized:\")\n",
    "        print(f\"   Trainable parameters: {sum(p.numel() for p in self.trainable_params):,}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "        print(f\"   Max epochs: {max_epochs}\")\n",
    "        print(f\"   Device: {device}\")\n",
    "    \n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {self.current_epoch + 1}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            predictions = outputs['predictions']\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.trainable_params, max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            current_accuracy = correct_predictions / total_samples\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{current_accuracy:.3f}',\n",
    "                'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'learning_rate': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "    \n",
    "    def validate(self) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc='Validating', leave=False):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                \n",
    "                total_loss += outputs['loss'].item()\n",
    "                all_predictions.extend(outputs['predictions'].cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': all_predictions,\n",
    "            'labels': all_labels\n",
    "        }\n",
    "    \n",
    "    def train(self) -> Dict:\n",
    "        print(f\"\\nüöÄ Starting training for {self.max_epochs} epochs...\")\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.current_epoch = epoch\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            train_metrics = self.train_epoch()\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            self.training_history['train_loss'].append(train_metrics['loss'])\n",
    "            self.training_history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "            self.training_history['val_loss'].append(val_metrics['loss'])\n",
    "            self.training_history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "            self.training_history['learning_rates'].append(train_metrics['learning_rate'])\n",
    "            self.training_history['epoch_times'].append(epoch_time)\n",
    "            \n",
    "            print(f\"\\nüìà Epoch {epoch + 1}/{self.max_epochs} Results:\")\n",
    "            print(f\"   Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['accuracy']:.3f}\")\n",
    "            print(f\"   Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.3f}\")\n",
    "            print(f\"   Time: {epoch_time:.1f}s\")\n",
    "            \n",
    "            if val_metrics['accuracy'] > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = val_metrics['accuracy']\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                self.patience_counter = 0\n",
    "                print(f\"   üéâ New best validation accuracy: {self.best_val_accuracy:.3f}\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                print(f\"   ‚è≥ No improvement for {self.patience_counter} epochs\")\n",
    "            \n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(f\"\\nüõë Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "        \n",
    "        total_training_time = time.time() - training_start_time\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training completed!\")\n",
    "        print(f\"   Total time: {total_training_time:.1f}s\")\n",
    "        print(f\"   Best validation accuracy: {self.best_val_accuracy:.3f}\")\n",
    "        \n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"   üì• Loaded best model weights\")\n",
    "        \n",
    "        return {\n",
    "            'best_val_accuracy': self.best_val_accuracy,\n",
    "            'total_time': total_training_time,\n",
    "            'epochs_trained': epoch + 1,\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Professional Training Class Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device and create trainer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "trainer = LoRAEmailTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_epochs=5,\n",
    "    patience=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Training Configuration:\")\n",
    "print(f\"   Model: DistilBERT + LoRA (rank=8)\")\n",
    "print(f\"   Dataset: {len(X_train)} train, {len(X_test)} test\")\n",
    "print(f\"   Batch size: 16 (train), 32 (test)\")\n",
    "print(f\"   Learning rate: 2e-4\")\n",
    "print(f\"   Early stopping: 3 epochs patience\")\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nüöÄ Let's train our LoRA email classifier!\")\n",
    "training_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Comprehensive Model Evaluation\n",
    "\n",
    "Now let's thoroughly evaluate our trained model with professional-grade metrics and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_evaluation(model, test_loader, category_mapping, device):\n",
    "    print(\"üî¨ Comprehensive Model Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            probabilities = F.softmax(outputs['logits'], dim=-1)\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    category_names = list(category_mapping.keys())\n",
    "    overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    print(f\"\\nüéØ Overall Accuracy: {overall_accuracy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Detailed Classification Report:\")\n",
    "    report = classification_report(all_labels, all_predictions, target_names=category_names, digits=3)\n",
    "    print(report)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=category_names, yticklabels=category_names, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    axes[0, 0].set_ylabel('Actual')\n",
    "    \n",
    "    # Per-class metrics\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_predictions, average=None)\n",
    "    \n",
    "    x = np.arange(len(category_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[0, 1].bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
    "    axes[0, 1].bar(x, recall, width, label='Recall', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Categories')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_title('Per-Class Performance Metrics')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(category_names, rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training curves\n",
    "    epochs = range(1, len(training_results['training_history']['train_loss']) + 1)\n",
    "    axes[1, 0].plot(epochs, training_results['training_history']['train_loss'], 'b-', label='Train Loss')\n",
    "    axes[1, 0].plot(epochs, training_results['training_history']['val_loss'], 'r-', label='Val Loss')\n",
    "    axes[1, 0].set_title('Training Loss Curves')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[1, 1].plot(epochs, training_results['training_history']['train_accuracy'], 'b-', label='Train Acc')\n",
    "    axes[1, 1].plot(epochs, training_results['training_history']['val_accuracy'], 'r-', label='Val Acc')\n",
    "    axes[1, 1].set_title('Training Accuracy Curves')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': overall_accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probabilities,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "# Perform evaluation\n",
    "eval_results = comprehensive_model_evaluation(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    category_mapping=generator.categories,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Real-World Demo\n",
    "\n",
    "Let's test our model with new emails to see how it performs in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_email_classifier_demo(model, tokenizer, category_mapping, device):\n",
    "    def classify_email(subject: str, body: str):\n",
    "        # Preprocess\n",
    "        preprocessor = EmailTextPreprocessor()\n",
    "        clean_subject = preprocessor.clean_text(subject)\n",
    "        clean_body = preprocessor.clean_text(body)\n",
    "        combined_text = clean_subject + ' ' + clean_body\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            combined_text, truncation=True, padding='max_length',\n",
    "            max_length=128, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(outputs['logits'], dim=-1)\n",
    "            predicted_class = torch.argmax(outputs['logits'], dim=-1)\n",
    "        \n",
    "        predicted_label = predicted_class.item()\n",
    "        predicted_category = list(category_mapping.keys())[list(category_mapping.values()).index(predicted_label)]\n",
    "        confidence = probabilities[0][predicted_label].item()\n",
    "        \n",
    "        # All probabilities\n",
    "        prob_dict = {}\n",
    "        for category, label in category_mapping.items():\n",
    "            prob_dict[category] = probabilities[0][label].item()\n",
    "        \n",
    "        return {\n",
    "            'predicted_category': predicted_category,\n",
    "            'confidence': confidence,\n",
    "            'all_probabilities': prob_dict\n",
    "        }\n",
    "    \n",
    "    return classify_email\n",
    "\n",
    "# Create demo\n",
    "email_classifier = create_email_classifier_demo(\n",
    "    model=model, tokenizer=model.tokenizer,\n",
    "    category_mapping=generator.categories, device=device\n",
    ")\n",
    "\n",
    "# Test emails\n",
    "test_emails = [\n",
    "    {\n",
    "        'subject': 'URGENT: Server Down - Revenue Impact!',\n",
    "        'body': 'The main server crashed and we are losing money every minute. Please fix this immediately!',\n",
    "        'expected': 'urgent'\n",
    "    },\n",
    "    {\n",
    "        'subject': 'Question about your pricing plans',\n",
    "        'body': 'Hi, I am interested in your enterprise plan for my company of 100 employees. Could you send me a quote?',\n",
    "        'expected': 'sales'\n",
    "    },\n",
    "    {\n",
    "        'subject': 'You have won $1 million dollars!!!',\n",
    "        'body': 'Congratulations! You are our lucky winner! Click here now to claim your amazing prize!',\n",
    "        'expected': 'spam'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing with New Emails:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "correct_predictions = 0\n",
    "for i, email in enumerate(test_emails):\n",
    "    print(f\"\\nüìß Test Email {i+1}:\")\n",
    "    print(f\"   Subject: {email['subject']}\")\n",
    "    print(f\"   Expected: {email['expected']}\")\n",
    "    \n",
    "    result = email_classifier(email['subject'], email['body'])\n",
    "    \n",
    "    print(f\"   Predicted: {result['predicted_category']} (confidence: {result['confidence']:.3f})\")\n",
    "    \n",
    "    is_correct = result['predicted_category'] == email['expected']\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "        print(f\"   Result: ‚úÖ CORRECT\")\n",
    "    else:\n",
    "        print(f\"   Result: ‚ùå INCORRECT\")\n",
    "    \n",
    "    print(f\"   All probabilities:\")\n",
    "    for category, prob in result['all_probabilities'].items():\n",
    "        marker = \"üëâ\" if category == result['predicted_category'] else \"  \"\n",
    "        print(f\"      {marker} {category}: {prob:.3f}\")\n",
    "\n",
    "demo_accuracy = correct_predictions / len(test_emails)\n",
    "print(f\"\\nüéØ Demo Accuracy: {correct_predictions}/{len(test_emails)} = {demo_accuracy:.3f}\")\n",
    "print(f\"‚úÖ Model is ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Training Summary\n",
    "\n",
    "Congratulations! You've successfully built and trained a production-ready LoRA email classifier. Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ **Key Achievements:**\n",
    "- **Built a complete training pipeline** with monitoring and early stopping\n",
    "- **Achieved strong performance** with minimal trainable parameters\n",
    "- **Created professional evaluation metrics** and visualizations\n",
    "- **Demonstrated real-world applicability** with practical examples\n",
    "- **Implemented best practices** for production ML systems\n",
    "\n",
    "### üéØ **Next Steps:**\n",
    "In Step 6, we'll compare our approach with alternatives and create a comprehensive business analysis.\n",
    "\n",
    "You're now ready to deploy this system or adapt it for other text classification tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}