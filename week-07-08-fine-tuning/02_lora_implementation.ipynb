{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Step 2: Building LoRA from Scratch\n",
    "\n",
    "## Week 7-8: Fine-tuning Implementation\n",
    "\n",
    "Now that you understand the theory, let's **build LoRA from scratch**! You'll implement every component and see exactly how it works.\n",
    "\n",
    "### üéØ What You'll Build:\n",
    "1. **Basic LoRA Layer** - Core implementation\n",
    "2. **LoRA Linear Layer** - Drop-in replacement for nn.Linear\n",
    "3. **Testing and Verification** - Make sure it works correctly\n",
    "4. **Performance Comparisons** - See the memory savings\n",
    "5. **Integration Examples** - How to use with real models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üöÄ Ready to build LoRA from scratch!\")\n",
    "print(f\"Using PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Part 1: Core LoRA Implementation\n",
    "\n",
    "Let's start by building the basic LoRA layer step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) Layer\n",
    "    \n",
    "    This implements: output = original_layer(x) + B @ A @ x\n",
    "    Where B and A are low-rank matrices we train\n",
    "    \"\"\"\n",
    "    def __init__(self, original_layer, rank=4, alpha=16, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the original layer (we'll freeze this)\n",
    "        self.original_layer = original_layer\n",
    "        \n",
    "        # Get dimensions from original layer\n",
    "        if hasattr(original_layer, 'in_features') and hasattr(original_layer, 'out_features'):\n",
    "            self.in_features = original_layer.in_features\n",
    "            self.out_features = original_layer.out_features\n",
    "        else:\n",
    "            raise ValueError(\"Original layer must be a Linear layer\")\n",
    "        \n",
    "        # LoRA parameters\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank  # This balances the adaptation strength\n",
    "        \n",
    "        # Create the LoRA matrices\n",
    "        # A: rank x in_features (initialized with random values)\n",
    "        # B: out_features x rank (initialized with zeros)\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, self.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.out_features, rank))\n",
    "        \n",
    "        # Optional dropout\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Freeze the original layer\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(f\"‚úÖ Created LoRA layer:\")\n",
    "        print(f\"   Original: {self.in_features} -> {self.out_features} ({self.in_features * self.out_features:,} params)\")\n",
    "        print(f\"   LoRA: rank={rank}, alpha={alpha}\")\n",
    "        print(f\"   A matrix: {rank} x {self.in_features} = {rank * self.in_features:,} params\")\n",
    "        print(f\"   B matrix: {self.out_features} x {rank} = {self.out_features * rank:,} params\")\n",
    "        print(f\"   Total trainable: {(rank * self.in_features) + (self.out_features * rank):,} params\")\n",
    "        print(f\"   üéØ Reduction: {(self.in_features * self.out_features) / ((rank * self.in_features) + (self.out_features * rank)):.1f}x\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: output = W‚ÇÄx + BAx\n",
    "        \n",
    "        We compute this efficiently as:\n",
    "        1. original_output = W‚ÇÄ @ x\n",
    "        2. lora_output = B @ (A @ x)  # Note: we don't materialize B@A\n",
    "        3. return original_output + scaling * lora_output\n",
    "        \"\"\"\n",
    "        # Original layer output (frozen weights)\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA adaptation path\n",
    "        # Step 1: A @ x (rank x batch_size x ...)\n",
    "        x_adapted = F.linear(x, self.lora_A)  # Same as self.lora_A @ x but more efficient\n",
    "        \n",
    "        # Apply dropout\n",
    "        x_adapted = self.dropout(x_adapted)\n",
    "        \n",
    "        # Step 2: B @ (A @ x)\n",
    "        lora_output = F.linear(x_adapted, self.lora_B.T)  # Transpose because F.linear expects transposed weights\n",
    "        \n",
    "        # Combine original and LoRA outputs with scaling\n",
    "        return original_output + lora_output * self.scaling\n",
    "    \n",
    "    def get_delta_weights(self):\n",
    "        \"\"\"\n",
    "        Get the actual weight changes (ŒîW = B @ A)\n",
    "        This is mainly for analysis - we don't compute this during forward pass\n",
    "        \"\"\"\n",
    "        return self.lora_B @ self.lora_A * self.scaling\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"\n",
    "        Merge LoRA weights into the original layer for deployment\n",
    "        This creates a single weight matrix: W_new = W‚ÇÄ + ŒîW\n",
    "        \"\"\"\n",
    "        if not hasattr(self.original_layer, 'weight'):\n",
    "            raise ValueError(\"Original layer must have 'weight' parameter\")\n",
    "        \n",
    "        delta_w = self.get_delta_weights()\n",
    "        self.original_layer.weight.data += delta_w\n",
    "        print(\"‚úÖ LoRA weights merged into original layer\")\n",
    "    \n",
    "    def get_parameter_info(self):\n",
    "        \"\"\"\n",
    "        Get detailed parameter information\n",
    "        \"\"\"\n",
    "        original_params = sum(p.numel() for p in self.original_layer.parameters())\n",
    "        lora_params = self.lora_A.numel() + self.lora_B.numel()\n",
    "        \n",
    "        return {\n",
    "            'original_params': original_params,\n",
    "            'lora_params': lora_params,\n",
    "            'total_params': original_params + lora_params,\n",
    "            'trainable_params': lora_params,\n",
    "            'reduction_factor': original_params / lora_params if lora_params > 0 else float('inf')\n",
    "        }\n",
    "\n",
    "# Test our LoRA implementation\n",
    "print(\"üß™ Testing LoRA Implementation:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create a sample linear layer\n",
    "original = nn.Linear(512, 256)\n",
    "\n",
    "# Wrap it with LoRA\n",
    "lora_layer = LoRALayer(original, rank=8, alpha=16)\n",
    "\n",
    "# Test with sample input\n",
    "x = torch.randn(2, 512)  # batch_size=2, features=512\n",
    "output = lora_layer(x)\n",
    "print(f\"\\n‚úÖ Forward pass successful!\")\n",
    "print(f\"   Input shape: {x.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Part 2: Let's Verify Our Implementation\n",
    "\n",
    "Now let's test that our LoRA layer works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lora_correctness():\n",
    "    \"\"\"\n",
    "    Test that our LoRA implementation works correctly\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing LoRA Correctness:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create a simple example\n",
    "    original = nn.Linear(4, 3)\n",
    "    lora = LoRALayer(original, rank=2, alpha=4)\n",
    "    \n",
    "    # Test input\n",
    "    x = torch.randn(1, 4)\n",
    "    \n",
    "    # Method 1: Use our LoRA layer\n",
    "    output_lora = lora(x)\n",
    "    \n",
    "    # Method 2: Manual computation for verification\n",
    "    original_output = original(x)\n",
    "    delta_w = lora.get_delta_weights()\n",
    "    manual_output = original_output + F.linear(x, delta_w)\n",
    "    \n",
    "    # Check if they're the same\n",
    "    difference = torch.abs(output_lora - manual_output).max().item()\n",
    "    \n",
    "    print(f\"‚úÖ Correctness test:\")\n",
    "    print(f\"   LoRA output: {output_lora.flatten()[:3]}...\")\n",
    "    print(f\"   Manual output: {manual_output.flatten()[:3]}...\")\n",
    "    print(f\"   Max difference: {difference:.8f}\")\n",
    "    print(f\"   {'‚úÖ PASSED' if difference < 1e-6 else '‚ùå FAILED'}\")\n",
    "    \n",
    "    return difference < 1e-6\n",
    "\n",
    "test_lora_correctness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 3: Memory and Parameter Analysis\n",
    "\n",
    "Let's see the real memory savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameter_efficiency():\n",
    "    \"\"\"\n",
    "    Analyze parameter efficiency across different layer sizes and ranks\n",
    "    \"\"\"\n",
    "    print(\"üìä Parameter Efficiency Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test different layer sizes (common in transformers)\n",
    "    layer_configs = [\n",
    "        (768, 768),    # BERT-base attention\n",
    "        (768, 3072),   # BERT-base FFN\n",
    "        (1024, 1024),  # Large attention\n",
    "        (1024, 4096),  # Large FFN\n",
    "        (2048, 2048),  # Very large attention\n",
    "    ]\n",
    "    \n",
    "    ranks = [4, 8, 16, 32, 64]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (in_dim, out_dim) in layer_configs:\n",
    "        original = nn.Linear(in_dim, out_dim)\n",
    "        original_params = sum(p.numel() for p in original.parameters())\n",
    "        \n",
    "        print(f\"\\nüîß Layer: {in_dim} -> {out_dim} ({original_params:,} params)\")\n",
    "        print(f\"   Rank  | LoRA Params | Reduction | Memory MB\")\n",
    "        print(f\"   -----|-----------|---------|----------\")\n",
    "        \n",
    "        for rank in ranks:\n",
    "            # Calculate LoRA parameters\n",
    "            lora_params = (in_dim * rank) + (out_dim * rank)\n",
    "            reduction = original_params / lora_params\n",
    "            memory_mb = lora_params * 4 / (1024 * 1024)  # 4 bytes per float32\n",
    "            \n",
    "            print(f\"   {rank:4d}  | {lora_params:9,} | {reduction:6.1f}x  | {memory_mb:7.2f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'layer_size': f\"{in_dim}x{out_dim}\",\n",
    "                'rank': rank,\n",
    "                'original_params': original_params,\n",
    "                'lora_params': lora_params,\n",
    "                'reduction': reduction,\n",
    "                'memory_mb': memory_mb\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "efficiency_results = analyze_parameter_efficiency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the efficiency results\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for easier visualization\n",
    "df = pd.DataFrame(efficiency_results)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Parameter reduction by rank for different layer sizes\n",
    "unique_layers = df['layer_size'].unique()\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_layers)))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "for i, layer in enumerate(unique_layers):\n",
    "    layer_data = df[df['layer_size'] == layer]\n",
    "    ax1.plot(layer_data['rank'], layer_data['reduction'], \n",
    "             marker='o', linewidth=2, label=layer, color=colors[i])\n",
    "\n",
    "ax1.set_xlabel('LoRA Rank')\n",
    "ax1.set_ylabel('Parameter Reduction Factor')\n",
    "ax1.set_title('Parameter Reduction vs Rank')\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Memory usage by rank\n",
    "ax2 = axes[0, 1]\n",
    "for i, layer in enumerate(unique_layers[:3]):  # Show only first 3 for clarity\n",
    "    layer_data = df[df['layer_size'] == layer]\n",
    "    ax2.plot(layer_data['rank'], layer_data['memory_mb'], \n",
    "             marker='s', linewidth=2, label=layer, color=colors[i])\n",
    "\n",
    "ax2.set_xlabel('LoRA Rank')\n",
    "ax2.set_ylabel('Memory Usage (MB)')\n",
    "ax2.set_title('Memory Usage vs Rank')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Comparison at rank=16\n",
    "rank16_data = df[df['rank'] == 16]\n",
    "ax3 = axes[1, 0]\n",
    "bars = ax3.bar(range(len(rank16_data)), rank16_data['reduction'])\n",
    "ax3.set_xlabel('Layer Configuration')\n",
    "ax3.set_ylabel('Parameter Reduction Factor')\n",
    "ax3.set_title('Reduction Factor at Rank=16')\n",
    "ax3.set_xticks(range(len(rank16_data)))\n",
    "ax3.set_xticklabels(rank16_data['layer_size'], rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, rank16_data['reduction'])):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{val:.1f}x', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Parameter count comparison\n",
    "ax4 = axes[1, 1]\n",
    "sample_layer = df[df['layer_size'] == '1024x1024']\n",
    "original_count = sample_layer['original_params'].iloc[0]\n",
    "\n",
    "ax4.plot(sample_layer['rank'], [original_count] * len(sample_layer), \n",
    "         'r--', linewidth=3, label='Original (Full Fine-tuning)')\n",
    "ax4.plot(sample_layer['rank'], sample_layer['lora_params'], \n",
    "         'b-o', linewidth=2, markersize=6, label='LoRA')\n",
    "\n",
    "ax4.set_xlabel('LoRA Rank')\n",
    "ax4.set_ylabel('Number of Parameters')\n",
    "ax4.set_title('Parameter Count: 1024x1024 Layer')\n",
    "ax4.set_yscale('log')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Rank 8-16 typically offers good balance (10-50x reduction)\")\n",
    "print(f\"   ‚Ä¢ Larger layers benefit more from LoRA\")\n",
    "print(f\"   ‚Ä¢ Memory savings are substantial even for small ranks\")\n",
    "print(f\"   ‚Ä¢ FFN layers (wide) get better compression than attention layers (square)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Part 4: Easy Integration with Existing Models\n",
    "\n",
    "Let's create a helper function to easily add LoRA to any model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lora_to_model(model, target_modules=None, rank=8, alpha=16, dropout=0.0):\n",
    "    \"\"\"\n",
    "    Add LoRA to specified modules in a model\n",
    "    \n",
    "    Args:\n",
    "        model: The model to modify\n",
    "        target_modules: List of module names to apply LoRA to (e.g., ['query', 'key', 'value'])\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA alpha scaling parameter\n",
    "        dropout: LoRA dropout probability\n",
    "    \"\"\"\n",
    "    if target_modules is None:\n",
    "        target_modules = ['query', 'key', 'value', 'dense']  # Common transformer module names\n",
    "    \n",
    "    lora_modules = []\n",
    "    original_param_count = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"üîß Adding LoRA to model:\")\n",
    "    print(f\"   Target modules: {target_modules}\")\n",
    "    print(f\"   Rank: {rank}, Alpha: {alpha}, Dropout: {dropout}\")\n",
    "    print(f\"   Original parameters: {original_param_count:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Walk through all modules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if this module should get LoRA\n",
    "        if any(target in name for target in target_modules) and isinstance(module, nn.Linear):\n",
    "            print(f\"   ‚úÖ Adding LoRA to: {name}\")\n",
    "            \n",
    "            # Replace the module with LoRA version\n",
    "            lora_module = LoRALayer(module, rank=rank, alpha=alpha, dropout=dropout)\n",
    "            \n",
    "            # Set the module in the model (this is a bit tricky with nested modules)\n",
    "            parent_module = model\n",
    "            module_parts = name.split('.')\n",
    "            \n",
    "            for part in module_parts[:-1]:\n",
    "                parent_module = getattr(parent_module, part)\n",
    "            \n",
    "            setattr(parent_module, module_parts[-1], lora_module)\n",
    "            lora_modules.append((name, lora_module))\n",
    "    \n",
    "    # Calculate new parameter counts\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "    print(f\"   LoRA modules added: {len(lora_modules)}\")\n",
    "    \n",
    "    return lora_modules\n",
    "\n",
    "# Test with a simple transformer-like model\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Attention layers\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Feed-forward layers\n",
    "        self.ffn1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.ffn2 = nn.Linear(d_model * 4, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Simplified forward pass\n",
    "        return x\n",
    "\n",
    "# Create and modify model\n",
    "model = SimpleTransformer(d_model=512, num_heads=8)\n",
    "lora_modules = add_lora_to_model(\n",
    "    model, \n",
    "    target_modules=['query', 'key', 'value'],  # Only attention, not FFN\n",
    "    rank=16, \n",
    "    alpha=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Part 5: Training with LoRA\n",
    "\n",
    "Let's see how to actually train a model with LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_lora_training():\n",
    "    \"\"\"\n",
    "    Demonstrate how to train with LoRA\n",
    "    \"\"\"\n",
    "    print(\"üöÄ LoRA Training Demonstration:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create a simple classification model\n",
    "    class SimpleClassifier(nn.Module):\n",
    "        def __init__(self, input_size=784, hidden_size=512, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "            self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.layer3 = nn.Linear(hidden_size, num_classes)\n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.layer1(x))\n",
    "            x = self.relu(self.layer2(x))\n",
    "            return self.layer3(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleClassifier()\n",
    "    \n",
    "    # Add LoRA to specific layers\n",
    "    lora_modules = add_lora_to_model(\n",
    "        model, \n",
    "        target_modules=['layer1', 'layer2'],  # Apply to first two layers\n",
    "        rank=8, \n",
    "        alpha=16\n",
    "    )\n",
    "    \n",
    "    # Create optimizer - only train LoRA parameters!\n",
    "    lora_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(lora_params, lr=1e-3)\n",
    "    \n",
    "    print(f\"\\nüéØ Training Setup:\")\n",
    "    print(f\"   Optimizer parameters: {sum(p.numel() for p in lora_params):,}\")\n",
    "    print(f\"   Memory for gradients: {sum(p.numel() for p in lora_params) * 4 / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Simulate training\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(3):  # Just a few steps for demonstration\n",
    "        # Create dummy batch\n",
    "        batch_x = torch.randn(32, 784)  # Batch of 32 samples\n",
    "        batch_y = torch.randint(0, 10, (32,))  # Random labels\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = F.cross_entropy(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass - only LoRA parameters get gradients!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"   Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    print(\"   ‚Ä¢ Only LoRA parameters were updated\")\n",
    "    print(\"   ‚Ä¢ Original weights remained frozen\")\n",
    "    print(\"   ‚Ä¢ Memory usage was much lower\")\n",
    "\n",
    "demonstrate_lora_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 6: Key Insights and Best Practices\n",
    "\n",
    "Let's summarize what you've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì Key Insights from LoRA Implementation:\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"1. üèóÔ∏è  ARCHITECTURE:\")\n",
    "print(\"   ‚Ä¢ LoRA adds trainable B and A matrices\")\n",
    "print(\"   ‚Ä¢ Original weights stay frozen\")\n",
    "print(\"   ‚Ä¢ Forward: output = W‚ÇÄx + Œ±/r * B(Ax)\")\n",
    "print()\n",
    "print(\"2. üìä  PARAMETER EFFICIENCY:\")\n",
    "print(\"   ‚Ä¢ 10-1000x fewer trainable parameters\")\n",
    "print(\"   ‚Ä¢ Memory savings scale with layer size\")\n",
    "print(\"   ‚Ä¢ Rank 8-16 usually sufficient for most tasks\")\n",
    "print()\n",
    "print(\"3. ‚öôÔ∏è  IMPLEMENTATION DETAILS:\")\n",
    "print(\"   ‚Ä¢ A initialized randomly, B initialized to zeros\")\n",
    "print(\"   ‚Ä¢ Alpha controls adaptation strength\")\n",
    "print(\"   ‚Ä¢ Don't materialize B@A during forward pass\")\n",
    "print()\n",
    "print(\"4. üöÄ  TRAINING BENEFITS:\")\n",
    "print(\"   ‚Ä¢ Much faster training (fewer parameters)\")\n",
    "print(\"   ‚Ä¢ Lower memory requirements\")\n",
    "print(\"   ‚Ä¢ Less prone to overfitting\")\n",
    "print(\"   ‚Ä¢ Can merge weights for deployment\")\n",
    "print()\n",
    "print(\"5. üéØ  BEST PRACTICES:\")\n",
    "print(\"   ‚Ä¢ Start with rank=8-16, alpha=16-32\")\n",
    "print(\"   ‚Ä¢ Apply to attention layers first\")\n",
    "print(\"   ‚Ä¢ Use dropout for regularization\")\n",
    "print(\"   ‚Ä¢ Monitor both original and LoRA parameter norms\")\n",
    "\n",
    "# Create a final summary visualization\n",
    "def create_summary_visualization():\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Memory comparison\n",
    "    methods = ['Full Fine-tuning', 'LoRA (r=8)', 'LoRA (r=16)', 'LoRA (r=32)']\n",
    "    memory_gb = [12.0, 0.8, 1.6, 3.2]  # Approximate for large model\n",
    "    colors = ['red', 'lightblue', 'blue', 'darkblue']\n",
    "    \n",
    "    bars1 = ax1.bar(methods, memory_gb, color=colors)\n",
    "    ax1.set_ylabel('Memory Usage (GB)')\n",
    "    ax1.set_title('Training Memory Comparison')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, memory_gb):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                f'{val:.1f}GB', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Parameter efficiency by layer type\n",
    "    layer_types = ['Small\\n(256x256)', 'Medium\\n(512x512)', 'Large\\n(1024x1024)', 'XLarge\\n(2048x2048)']\n",
    "    reductions = [16, 32, 64, 128]  # Approximate reduction factors for rank=16\n",
    "    \n",
    "    bars2 = ax2.bar(layer_types, reductions, color='green', alpha=0.7)\n",
    "    ax2.set_ylabel('Parameter Reduction Factor')\n",
    "    ax2.set_title('LoRA Efficiency by Layer Size (rank=16)')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    for bar, val in zip(bars2, reductions):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, \n",
    "                f'{val}x', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Rank vs performance trade-off (conceptual)\n",
    "    ranks = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    performance = [0.7, 0.8, 0.85, 0.9, 0.95, 0.97, 0.98, 0.99]  # Conceptual relative performance\n",
    "    efficiency = [1/r for r in ranks]  # Inverse of rank (efficiency)\n",
    "    \n",
    "    ax3_twin = ax3.twinx()\n",
    "    line1 = ax3.plot(ranks, performance, 'b-o', label='Performance', linewidth=2)\n",
    "    line2 = ax3_twin.plot(ranks, efficiency, 'r-s', label='Efficiency (1/rank)', linewidth=2)\n",
    "    \n",
    "    ax3.set_xlabel('LoRA Rank')\n",
    "    ax3.set_ylabel('Relative Performance', color='blue')\n",
    "    ax3_twin.set_ylabel('Efficiency (1/rank)', color='red')\n",
    "    ax3.set_title('Performance vs Efficiency Trade-off')\n",
    "    ax3.set_xscale('log', base=2)\n",
    "    \n",
    "    # Add legend\n",
    "    lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "    ax3.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "    \n",
    "    # 4. LoRA architecture diagram\n",
    "    ax4.text(0.5, 0.9, 'LoRA Forward Pass', ha='center', va='top', fontsize=14, fontweight='bold')\n",
    "    ax4.text(0.5, 0.8, 'y = W‚ÇÄx + (Œ±/r) √ó B √ó (A √ó x)', ha='center', va='center', \n",
    "            fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    ax4.text(0.5, 0.65, '‚Üì', ha='center', va='center', fontsize=20)\n",
    "    ax4.text(0.1, 0.5, 'Original\\nW‚ÇÄx\\n(frozen)', ha='center', va='center', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "    ax4.text(0.5, 0.5, '+', ha='center', va='center', fontsize=20)\n",
    "    ax4.text(0.9, 0.5, 'LoRA\\nBAx\\n(trainable)', ha='center', va='center', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "    ax4.text(0.5, 0.3, '‚Üì', ha='center', va='center', fontsize=20)\n",
    "    ax4.text(0.5, 0.15, 'Final Output', ha='center', va='center', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"gold\"))\n",
    "    \n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.set_xticks([])\n",
    "    ax4.set_yticks([])\n",
    "    ax4.set_title('LoRA Architecture')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_summary_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built LoRA from scratch! Here's what you accomplished:\n",
    "\n",
    "### ‚úÖ **What You Built:**\n",
    "1. **Complete LoRA implementation** - Core layer with all features\n",
    "2. **Integration helpers** - Easy functions to add LoRA to any model\n",
    "3. **Training examples** - How to train only LoRA parameters\n",
    "4. **Analysis tools** - Parameter efficiency and memory calculations\n",
    "\n",
    "### üß† **What You Learned:**\n",
    "1. **Mathematical foundation** - How B√óA decomposition works\n",
    "2. **Implementation details** - Initialization, scaling, efficient computation\n",
    "3. **Parameter efficiency** - Why LoRA saves 10-1000x parameters\n",
    "4. **Practical aspects** - How to actually use LoRA in training\n",
    "\n",
    "### üöÄ **Next Steps:**\n",
    "In **Step 3**, we'll:\n",
    "- Use LoRA with real pre-trained models (BERT, RoBERTa)\n",
    "- Implement QLoRA for even more memory savings\n",
    "- Build a complete email classification system\n",
    "\n",
    "### üí° **Quick Self-Check:**\n",
    "- Can you explain why LoRA is memory efficient?\n",
    "- Do you understand the B√óA decomposition?\n",
    "- Can you implement a basic LoRA layer from scratch?\n",
    "\n",
    "If yes, you're ready for Step 3! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}