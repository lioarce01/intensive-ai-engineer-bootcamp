{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Step 1: Understanding LoRA - Concepts and Theory\n",
    "\n",
    "## Week 7-8: Fine-tuning with LoRA/QLoRA and PEFT\n",
    "\n",
    "Welcome to your first lesson on **LoRA (Low-Rank Adaptation)**! This notebook will teach you the fundamental concepts step by step.\n",
    "\n",
    "### ğŸ¯ What You'll Learn:\n",
    "1. **What is fine-tuning and why it matters**\n",
    "2. **The problem with traditional fine-tuning**\n",
    "3. **What is LoRA and why it's revolutionary**\n",
    "4. **The mathematics behind LoRA**\n",
    "5. **Visual intuition with simple examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Part 1: What is Fine-tuning?\n",
    "\n",
    "Imagine you have a **smart friend who knows everything** (pre-trained model) but you want to teach them **your specific job** (task-specific knowledge).\n",
    "\n",
    "### Examples:\n",
    "- **General Model**: Knows language, grammar, facts\n",
    "- **Your Task**: Classify emails as spam/not spam for YOUR company\n",
    "- **Fine-tuning**: Teaching the model your specific email patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ‰ Libraries loaded! Ready to learn LoRA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” Part 2: The Problem with Traditional Fine-tuning\n",
    "\n",
    "Let's understand the problem with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate a large language model\n",
    "class SimpleTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model=768):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(d_model, d_model)  # Simplified\n",
    "        self.ffn1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.ffn2 = nn.Linear(d_model * 4, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x  # Simplified for demonstration\n",
    "\n",
    "# Create a model similar to BERT-base\n",
    "model = SimpleTransformerLayer(d_model=768)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"ğŸ“Š Total Parameters: {total_params:,}\")\n",
    "print(f\"ğŸ’¾ Memory for full fine-tuning: ~{total_params * 4 / 1e6:.1f} MB (just weights)\")\n",
    "print(f\"ğŸ’¾ Memory for gradients: ~{total_params * 4 / 1e6:.1f} MB (additional)\")\n",
    "print(f\"ğŸ’¾ Total training memory: ~{total_params * 8 / 1e6:.1f} MB per layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ˜± The Problems:\n",
    "1. **Memory Explosion**: Need to store gradients for ALL parameters\n",
    "2. **Overfitting**: Too many parameters for small datasets\n",
    "3. **Catastrophic Forgetting**: Model might forget its original knowledge\n",
    "4. **Storage**: Need to save entire model for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the memory problem\n",
    "model_sizes = ['BERT-base', 'BERT-large', 'GPT-2', 'GPT-3.5']\n",
    "params_millions = [110, 340, 1500, 175000]  # Approximate parameter counts\n",
    "\n",
    "# Calculate memory requirements (MB)\n",
    "full_finetune_memory = [p * 8 / 1000 for p in params_millions]  # 8 bytes per param (weights + gradients)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_sizes, full_finetune_memory, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Memory Required (MB)')\n",
    "plt.title('ğŸ’¥ Memory Explosion: Full Fine-tuning Requirements')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, val) in enumerate(zip(bars, full_finetune_memory)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, \n",
    "             f'{val:,.0f} MB', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ”¥ GPT-3.5 full fine-tuning would need ~{full_finetune_memory[-1]/1000:.0f} GB of memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Part 3: Enter LoRA - The Game Changer\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is like teaching someone **only the differences** instead of relearning everything!\n",
    "\n",
    "### ğŸ§  The Key Insight:\n",
    "When we fine-tune models, the **changes to the weights are actually very simple** (low-rank). We don't need to change everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the LoRA concept with a simple example\n",
    "def visualize_lora_concept():\n",
    "    # Original weight matrix (pre-trained)\n",
    "    W_original = np.random.randn(4, 4)\n",
    "    \n",
    "    # Traditional fine-tuning: change the entire matrix\n",
    "    W_finetuned = W_original + np.random.randn(4, 4) * 0.1\n",
    "    \n",
    "    # LoRA: represent changes as two smaller matrices\n",
    "    rank = 2  # Much smaller than 4\n",
    "    B = np.random.randn(4, rank) * 0.1  # 4 x 2\n",
    "    A = np.random.randn(rank, 4) * 0.1  # 2 x 4\n",
    "    \n",
    "    # LoRA adaptation: W_new = W_original + B @ A\n",
    "    W_lora = W_original + B @ A\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    im1 = axes[0].imshow(W_original, cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[0].set_title('Original Weights\\n(Pre-trained)')\n",
    "    axes[0].set_xlabel(f'Parameters: {W_original.size}')\n",
    "    \n",
    "    axes[1].imshow(B, cmap='RdBu', vmin=-0.3, vmax=0.3)\n",
    "    axes[1].set_title('LoRA Matrix B\\n(Trainable)')\n",
    "    axes[1].set_xlabel(f'Parameters: {B.size}')\n",
    "    \n",
    "    axes[2].imshow(A, cmap='RdBu', vmin=-0.3, vmax=0.3)\n",
    "    axes[2].set_title('LoRA Matrix A\\n(Trainable)')\n",
    "    axes[2].set_xlabel(f'Parameters: {A.size}')\n",
    "    \n",
    "    im4 = axes[3].imshow(W_lora, cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[3].set_title('Final Weights\\n(Original + LoRA)')\n",
    "    axes[3].set_xlabel(f'Same size: {W_lora.size}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Parameter comparison\n",
    "    original_params = W_original.size\n",
    "    lora_params = B.size + A.size\n",
    "    reduction = original_params / lora_params\n",
    "    \n",
    "    print(f\"ğŸ“Š Parameter Comparison:\")\n",
    "    print(f\"   Original matrix: {original_params} parameters\")\n",
    "    print(f\"   LoRA matrices: {lora_params} parameters\")\n",
    "    print(f\"   ğŸ¯ Reduction: {reduction:.1f}x fewer parameters!\")\n",
    "    \n",
    "    return W_original, B, A, W_lora\n",
    "\n",
    "W_orig, B, A, W_lora = visualize_lora_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Part 4: The Mathematics of LoRA\n",
    "\n",
    "Let's understand the math step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¢ LoRA Mathematics:\")\n",
    "print(\"=\"*50)\n",
    "print()\n",
    "print(\"1. Original linear layer:\")\n",
    "print(\"   y = Wâ‚€ Â· x\")\n",
    "print(\"   where Wâ‚€ is pre-trained weights\")\n",
    "print()\n",
    "print(\"2. Traditional fine-tuning:\")\n",
    "print(\"   y = (Wâ‚€ + Î”W) Â· x\")\n",
    "print(\"   where Î”W has same size as Wâ‚€\")\n",
    "print()\n",
    "print(\"3. LoRA insight:\")\n",
    "print(\"   Î”W â‰ˆ B Â· A  (low-rank approximation)\")\n",
    "print(\"   where B âˆˆ â„áµˆË£Ê³, A âˆˆ â„Ê³Ë£áµ, r << min(d,k)\")\n",
    "print()\n",
    "print(\"4. LoRA forward pass:\")\n",
    "print(\"   y = Wâ‚€ Â· x + B Â· A Â· x\")\n",
    "print(\"   y = Wâ‚€ Â· x + B Â· (A Â· x)\")\n",
    "print()\n",
    "print(\"5. Key insight:\")\n",
    "print(\"   - Wâ‚€ stays frozen (no gradients)\")\n",
    "print(\"   - Only train B and A (much smaller!)\")\n",
    "print(\"   - r is the 'rank' - controls adaptation capacity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how rank affects parameter reduction\n",
    "def analyze_rank_impact(original_size=1024):\n",
    "    ranks = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    original_params = original_size * original_size\n",
    "    \n",
    "    lora_params = []\n",
    "    reductions = []\n",
    "    \n",
    "    for r in ranks:\n",
    "        # B: original_size x r, A: r x original_size\n",
    "        lora_param_count = (original_size * r) + (r * original_size)\n",
    "        lora_param_count = 2 * original_size * r  # Simplified\n",
    "        \n",
    "        lora_params.append(lora_param_count)\n",
    "        reductions.append(original_params / lora_param_count)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Parameter count comparison\n",
    "    ax1.plot(ranks, [original_params] * len(ranks), 'r--', label='Full Fine-tuning', linewidth=2)\n",
    "    ax1.plot(ranks, lora_params, 'b-o', label='LoRA', linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('LoRA Rank (r)')\n",
    "    ax1.set_ylabel('Number of Trainable Parameters')\n",
    "    ax1.set_title(f'Parameter Count: {original_size}x{original_size} Layer')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reduction factor\n",
    "    ax2.plot(ranks, reductions, 'g-o', linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('LoRA Rank (r)')\n",
    "    ax2.set_ylabel('Parameter Reduction Factor')\n",
    "    ax2.set_title('Memory Savings with LoRA')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (r, red) in enumerate(zip(ranks[::2], reductions[::2])):\n",
    "        ax2.annotate(f'{red:.1f}x', (r, red), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ¯ Key Insights for {original_size}x{original_size} layer:\")\n",
    "    print(f\"   Full fine-tuning: {original_params:,} parameters\")\n",
    "    print(f\"   LoRA rank=8: {lora_params[3]:,} parameters ({reductions[3]:.1f}x reduction)\")\n",
    "    print(f\"   LoRA rank=16: {lora_params[4]:,} parameters ({reductions[4]:.1f}x reduction)\")\n",
    "    \n",
    "analyze_rank_impact(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 5: Why Does LoRA Work So Well?\n",
    "\n",
    "There are deep theoretical and practical reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate the \"intrinsic rank\" hypothesis\n",
    "def demonstrate_intrinsic_rank():\n",
    "    print(\"ğŸ”¬ The Intrinsic Rank Hypothesis\")\n",
    "    print(\"=\"*40)\n",
    "    print()\n",
    "    print(\"Theory: When we fine-tune models, the weight changes (Î”W)\")\n",
    "    print(\"have a low 'intrinsic rank' - meaning they can be well\")\n",
    "    print(\"approximated by the product of two smaller matrices.\")\n",
    "    print()\n",
    "    \n",
    "    # Simulate a realistic weight update during fine-tuning\n",
    "    np.random.seed(42)\n",
    "    size = 512\n",
    "    \n",
    "    # Create a typical weight update matrix (this would come from actual fine-tuning)\n",
    "    # In reality, this tends to have low rank due to the optimization dynamics\n",
    "    true_rank = 16  # Much smaller than 512\n",
    "    U = np.random.randn(size, true_rank)\n",
    "    V = np.random.randn(true_rank, size)\n",
    "    delta_W = U @ V  # This is inherently rank-16\n",
    "    \n",
    "    # Add some noise to make it more realistic\n",
    "    delta_W += np.random.randn(size, size) * 0.01\n",
    "    \n",
    "    # Perform SVD to analyze the rank structure\n",
    "    U_svd, s, Vt = np.linalg.svd(delta_W)\n",
    "    \n",
    "    # Plot singular values\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(s, 'b-', linewidth=2)\n",
    "    plt.axvline(x=true_rank, color='r', linestyle='--', label=f'True rank: {true_rank}')\n",
    "    plt.xlabel('Singular Value Index')\n",
    "    plt.ylabel('Singular Value Magnitude')\n",
    "    plt.title('Singular Values of Weight Update Î”W')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show cumulative explained variance\n",
    "    plt.subplot(1, 2, 2)\n",
    "    explained_var = np.cumsum(s**2) / np.sum(s**2)\n",
    "    plt.plot(explained_var, 'g-', linewidth=2)\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "    plt.axvline(x=np.where(explained_var >= 0.95)[0][0], color='r', linestyle='--')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('How Much Information is in Low Ranks?')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    rank_95 = np.where(explained_var >= 0.95)[0][0]\n",
    "    print(f\"ğŸ“Š Results:\")\n",
    "    print(f\"   Matrix size: {size}x{size} = {size**2:,} parameters\")\n",
    "    print(f\"   95% of information captured by rank: {rank_95}\")\n",
    "    print(f\"   LoRA with rank {rank_95}: {2*size*rank_95:,} parameters\")\n",
    "    print(f\"   ğŸ¯ Compression ratio: {(size**2)/(2*size*rank_95):.1f}x\")\n",
    "\n",
    "demonstrate_intrinsic_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Part 6: LoRA Benefits Summary\n",
    "\n",
    "Let's summarize why LoRA is revolutionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison\n",
    "def create_comparison_table():\n",
    "    import pandas as pd\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Aspect': [\n",
    "            'Trainable Parameters',\n",
    "            'Memory Usage',\n",
    "            'Training Speed',\n",
    "            'Storage per Task',\n",
    "            'Risk of Catastrophic Forgetting',\n",
    "            'Overfitting Risk',\n",
    "            'Task Performance',\n",
    "            'Implementation Complexity'\n",
    "        ],\n",
    "        'Full Fine-tuning': [\n",
    "            'All parameters (100%)',\n",
    "            'High (gradients for all)',\n",
    "            'Slow',\n",
    "            'Full model size',\n",
    "            'High',\n",
    "            'High (small datasets)',\n",
    "            'Excellent',\n",
    "            'Simple'\n",
    "        ],\n",
    "        'LoRA': [\n",
    "            '0.1-1% of parameters',\n",
    "            'Low (small adapters)',\n",
    "            'Fast',\n",
    "            'Only adapter weights',\n",
    "            'Low',\n",
    "            'Low (regularized)',\n",
    "            'Nearly identical',\n",
    "            'Moderate'\n",
    "        ],\n",
    "        'Winner': [\n",
    "            'ğŸ† LoRA',\n",
    "            'ğŸ† LoRA',\n",
    "            'ğŸ† LoRA',\n",
    "            'ğŸ† LoRA',\n",
    "            'ğŸ† LoRA',\n",
    "            'ğŸ† LoRA',\n",
    "            'ğŸ¤ Tie',\n",
    "            'ğŸ† Full FT'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"ğŸ“Š LoRA vs Full Fine-tuning Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    print(df.to_string(index=False))\n",
    "    print()\n",
    "    print(\"ğŸ¯ Winner: LoRA wins in 6/8 categories!\")\n",
    "\n",
    "create_comparison_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways from Step 1\n",
    "\n",
    "### What You've Learned:\n",
    "1. **Fine-tuning Problem**: Traditional approach requires too much memory and risks overfitting\n",
    "2. **LoRA Solution**: Represent weight changes as low-rank matrices (BÃ—A)\n",
    "3. **Mathematical Insight**: Î”W = BÃ—A where B and A are much smaller\n",
    "4. **Practical Benefits**: 10-1000x fewer parameters, faster training, less overfitting\n",
    "5. **Theoretical Foundation**: Weight updates during fine-tuning are naturally low-rank\n",
    "\n",
    "### ğŸš€ Next Step: \n",
    "In **Step 2**, we'll implement LoRA from scratch and see it working with real code!\n",
    "\n",
    "### ğŸ’¡ Quick Check:\n",
    "Can you explain to yourself:\n",
    "- Why is LoRA more memory efficient?\n",
    "- What does \"rank\" mean in LoRA?\n",
    "- How does LoRA prevent catastrophic forgetting?\n",
    "\n",
    "If you can answer these, you're ready for Step 2! ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}