{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📧 Step 4: Real Email Classification with LoRA\n",
    "\n",
    "## Week 7-8: Practical LoRA Application\n",
    "\n",
    "Now we'll build the **real project** required by the curriculum: a practical email/ticket classifier using LoRA fine-tuning.\n",
    "\n",
    "### 🎯 What You'll Learn:\n",
    "1. **Real dataset handling** - Working with actual email data\n",
    "2. **Data preprocessing** - Cleaning and preparing text for training\n",
    "3. **Dataset augmentation** - Creating more training data\n",
    "4. **Business problem solving** - Email classification is used in real companies\n",
    "5. **Production considerations** - How to make this work in the real world\n",
    "\n",
    "### 🏢 Business Context:\n",
    "Email classification is crucial for:\n",
    "- **Customer Support**: Routing tickets to right teams\n",
    "- **Sales**: Prioritizing leads and opportunities  \n",
    "- **Security**: Detecting spam and phishing\n",
    "- **Organization**: Auto-categorizing internal communications\n",
    "\n",
    "This is a **real problem** that AI engineers solve every day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for our email classification project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"📧 Email Classification Project Setup Complete!\")\n",
    "print(f\"Using PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗂️ Part 1: Creating Realistic Email Dataset\n",
    "\n",
    "Since we need a **real email classification dataset**, we'll create one that mimics actual business emails.\n",
    "\n",
    "### 🧠 Learning Objective:\n",
    "- **Data generation for ML**: Often you need to create or augment datasets\n",
    "- **Business domain understanding**: What makes emails different categories?\n",
    "- **Text preprocessing**: How to clean and prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generates realistic email dataset for classification training\n",
    "    \n",
    "    This teaches us:\n",
    "    1. How to structure business problems\n",
    "    2. What makes good training data\n",
    "    3. Domain knowledge importance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Email categories we'll classify (common business use case)\n",
    "        self.categories = {\n",
    "            'urgent': 0,      # High priority - needs immediate attention\n",
    "            'support': 1,     # Customer support requests\n",
    "            'sales': 2,       # Sales inquiries and leads\n",
    "            'spam': 3,        # Spam/promotional emails\n",
    "            'normal': 4       # Regular business communication\n",
    "        }\n",
    "        \n",
    "        # Templates for each category (this mimics real business emails)\n",
    "        self.email_templates = {\n",
    "            'urgent': [\n",
    "                \"URGENT: System is down and affecting all customers. Please respond immediately.\",\n",
    "                \"CRITICAL: Security breach detected. Need immediate action from the team.\",\n",
    "                \"EMERGENCY: Client meeting in 1 hour, presentation file corrupted. Help needed now!\",\n",
    "                \"URGENT RESPONSE NEEDED: Major bug in production causing revenue loss.\",\n",
    "                \"CRITICAL ISSUE: Payment system not working, customers cannot complete purchases.\",\n",
    "                \"IMMEDIATE ACTION REQUIRED: Server outage affecting 1000+ users.\",\n",
    "                \"URGENT: CEO wants status update on project by end of day.\",\n",
    "                \"EMERGENCY: Database corruption, need to restore from backup immediately.\"\n",
    "            ],\n",
    "            'support': [\n",
    "                \"Hi, I'm having trouble logging into my account. Can you help me reset my password?\",\n",
    "                \"Hello, the software keeps crashing when I try to export data. What should I do?\",\n",
    "                \"I can't find the feature you mentioned in the tutorial. Could you guide me?\",\n",
    "                \"The mobile app is not syncing with my desktop. How can I fix this?\",\n",
    "                \"I accidentally deleted important files. Is there a way to recover them?\",\n",
    "                \"The payment didn't go through but money was deducted. Please check my account.\",\n",
    "                \"I need help setting up the integration with our existing system.\",\n",
    "                \"The dashboard is showing incorrect data. Can someone look into this?\"\n",
    "            ],\n",
    "            'sales': [\n",
    "                \"I'm interested in your enterprise plan. Can you send me pricing information?\",\n",
    "                \"We're looking for a solution for our team of 50 people. What do you recommend?\",\n",
    "                \"Could we schedule a demo to see how your product fits our needs?\",\n",
    "                \"I saw your product at the conference. Can we discuss a potential partnership?\",\n",
    "                \"Our company is expanding and we need a scalable solution. Let's talk.\",\n",
    "                \"I'm evaluating different vendors. What makes your product unique?\",\n",
    "                \"We have a budget approved for this quarter. When can we start implementation?\",\n",
    "                \"Can you provide a quote for 200 user licenses with premium support?\"\n",
    "            ],\n",
    "            'spam': [\n",
    "                \"Congratulations! You've won $1,000,000! Click here to claim your prize now!\",\n",
    "                \"AMAZING OFFER: Buy one get one free! Limited time only! Act now!\",\n",
    "                \"Your account will be suspended unless you verify immediately. Click this link.\",\n",
    "                \"Hot singles in your area want to meet you! Join now for free!\",\n",
    "                \"Make money from home! No experience needed! Start earning today!\",\n",
    "                \"FINAL NOTICE: Your warranty is about to expire. Renew now for 50% off!\",\n",
    "                \"Free iPhone 14! You've been selected! Claim within 24 hours!\",\n",
    "                \"URGENT: Your PayPal account has been compromised. Verify now!\"\n",
    "            ],\n",
    "            'normal': [\n",
    "                \"Thanks for the meeting yesterday. Here are the notes we discussed.\",\n",
    "                \"The project timeline looks good. Let's proceed with the next phase.\",\n",
    "                \"I've reviewed the documents and have a few questions. Can we chat tomorrow?\",\n",
    "                \"The team meeting is scheduled for Friday at 2 PM in conference room B.\",\n",
    "                \"Please find attached the monthly report for your review.\",\n",
    "                \"I'll be out of office next week. John will handle any urgent matters.\",\n",
    "                \"The client approved the proposal. We can start implementation next month.\",\n",
    "                \"Can you send me the latest version of the design mockups?\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Subject line patterns (important for email classification)\n",
    "        self.subject_patterns = {\n",
    "            'urgent': ['URGENT:', 'CRITICAL:', 'EMERGENCY:', 'IMMEDIATE:', 'ASAP:'],\n",
    "            'support': ['Help:', 'Issue:', 'Problem:', 'Question:', 'Trouble:'],\n",
    "            'sales': ['Inquiry:', 'Demo:', 'Partnership:', 'Quote:', 'Pricing:'],\n",
    "            'spam': ['FREE!', 'WIN!', 'AMAZING!', 'CONGRATULATIONS!', 'LIMITED TIME!'],\n",
    "            'normal': ['Re:', 'Meeting:', 'Update:', 'FYI:', 'Report:']\n",
    "        }\n",
    "    \n",
    "    def generate_email(self, category: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Generate a single email for the given category\n",
    "        \n",
    "        This teaches us how to create realistic training data\n",
    "        \"\"\"\n",
    "        # Choose random template and subject pattern\n",
    "        body = random.choice(self.email_templates[category])\n",
    "        subject_prefix = random.choice(self.subject_patterns[category])\n",
    "        \n",
    "        # Add some variation to make it more realistic\n",
    "        variations = [\n",
    "            lambda text: text + \" Please let me know if you need any clarification.\",\n",
    "            lambda text: \"Hi team, \" + text.lower(),\n",
    "            lambda text: text + \" Thanks for your help!\",\n",
    "            lambda text: text + \" Best regards, \" + random.choice([\"John\", \"Sarah\", \"Mike\", \"Emma\"]),\n",
    "            lambda text: text  # No variation\n",
    "        ]\n",
    "        \n",
    "        # Apply random variation\n",
    "        if random.random() > 0.3:  # 70% chance of variation\n",
    "            body = random.choice(variations)(body)\n",
    "        \n",
    "        # Create subject line\n",
    "        subject = f\"{subject_prefix} {body.split('.')[0][:50]}...\"\n",
    "        \n",
    "        return {\n",
    "            'subject': subject,\n",
    "            'body': body,\n",
    "            'category': category,\n",
    "            'label': self.categories[category]\n",
    "        }\n",
    "    \n",
    "    def generate_dataset(self, samples_per_category: int = 200) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a complete dataset\n",
    "        \n",
    "        This teaches us about balanced datasets and data distribution\n",
    "        \"\"\"\n",
    "        print(f\"🏗️  Generating realistic email dataset...\")\n",
    "        print(f\"   Categories: {list(self.categories.keys())}\")\n",
    "        print(f\"   Samples per category: {samples_per_category}\")\n",
    "        \n",
    "        emails = []\n",
    "        \n",
    "        for category in self.categories.keys():\n",
    "            for _ in range(samples_per_category):\n",
    "                email = self.generate_email(category)\n",
    "                emails.append(email)\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        random.shuffle(emails)\n",
    "        \n",
    "        df = pd.DataFrame(emails)\n",
    "        \n",
    "        print(f\"✅ Dataset generated successfully!\")\n",
    "        print(f\"   Total samples: {len(df)}\")\n",
    "        print(f\"   Class distribution:\")\n",
    "        \n",
    "        class_counts = df['category'].value_counts()\n",
    "        for category, count in class_counts.items():\n",
    "            print(f\"      {category}: {count} samples\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Generate our dataset\n",
    "generator = EmailDatasetGenerator()\n",
    "email_df = generator.generate_dataset(samples_per_category=150)  # 750 total samples\n",
    "\n",
    "# Display sample emails\n",
    "print(\"\\n📧 Sample Emails:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category in ['urgent', 'support', 'sales', 'spam', 'normal']:\n",
    "    sample = email_df[email_df['category'] == category].iloc[0]\n",
    "    print(f\"\\n🏷️  {category.upper()}:\")\n",
    "    print(f\"   Subject: {sample['subject'][:60]}...\")\n",
    "    print(f\"   Body: {sample['body'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Part 2: Data Analysis and Preprocessing\n",
    "\n",
    "### 🧠 Learning Objective:\n",
    "**Understanding your data is crucial for ML success!** We'll analyze patterns, clean text, and prepare for training.\n",
    "\n",
    "This teaches us:\n",
    "- **Exploratory Data Analysis (EDA)**: Understanding data patterns\n",
    "- **Text preprocessing**: Essential for NLP tasks\n",
    "- **Feature engineering**: Creating better inputs for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_email_dataset(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of our email dataset\n",
    "    \n",
    "    This teaches us how to understand our data before training\n",
    "    \"\"\"\n",
    "    print(\"📊 Email Dataset Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Categories: {df['category'].unique()}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    df['subject_length'] = df['subject'].str.len()\n",
    "    df['body_length'] = df['body'].str.len()\n",
    "    df['total_length'] = df['subject_length'] + df['body_length']\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Class distribution\n",
    "    category_counts = df['category'].value_counts()\n",
    "    axes[0,0].bar(category_counts.index, category_counts.values, color='skyblue')\n",
    "    axes[0,0].set_title('Email Category Distribution')\n",
    "    axes[0,0].set_xlabel('Category')\n",
    "    axes[0,0].set_ylabel('Count')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Text length distribution by category\n",
    "    for category in df['category'].unique():\n",
    "        category_data = df[df['category'] == category]['total_length']\n",
    "        axes[0,1].hist(category_data, alpha=0.7, label=category, bins=20)\n",
    "    \n",
    "    axes[0,1].set_title('Text Length Distribution by Category')\n",
    "    axes[0,1].set_xlabel('Total Text Length (characters)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Average text lengths by category\n",
    "    avg_lengths = df.groupby('category')['total_length'].mean().sort_values(ascending=False)\n",
    "    axes[1,0].bar(avg_lengths.index, avg_lengths.values, color='lightcoral')\n",
    "    axes[1,0].set_title('Average Text Length by Category')\n",
    "    axes[1,0].set_xlabel('Category')\n",
    "    axes[1,0].set_ylabel('Average Length (characters)')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Word count analysis\n",
    "    df['word_count'] = df['body'].str.split().str.len()\n",
    "    word_stats = df.groupby('category')['word_count'].agg(['mean', 'std']).round(2)\n",
    "    \n",
    "    x_pos = np.arange(len(word_stats.index))\n",
    "    axes[1,1].bar(x_pos, word_stats['mean'], yerr=word_stats['std'], \n",
    "                  capsize=5, color='lightgreen', alpha=0.8)\n",
    "    axes[1,1].set_title('Average Word Count by Category')\n",
    "    axes[1,1].set_xlabel('Category')\n",
    "    axes[1,1].set_ylabel('Average Word Count')\n",
    "    axes[1,1].set_xticks(x_pos)\n",
    "    axes[1,1].set_xticklabels(word_stats.index, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n📈 Detailed Statistics:\")\n",
    "    print(f\"Average text lengths by category:\")\n",
    "    for category, length in avg_lengths.items():\n",
    "        print(f\"   {category}: {length:.1f} characters\")\n",
    "    \n",
    "    print(f\"\\nWord count statistics:\")\n",
    "    for category in word_stats.index:\n",
    "        mean_words = word_stats.loc[category, 'mean']\n",
    "        std_words = word_stats.loc[category, 'std']\n",
    "        print(f\"   {category}: {mean_words:.1f} ± {std_words:.1f} words\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze our dataset\n",
    "email_df = analyze_email_dataset(email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesses email text for better model performance\n",
    "    \n",
    "    This teaches us essential text preprocessing techniques:\n",
    "    1. Cleaning and normalization\n",
    "    2. Feature engineering for emails\n",
    "    3. Preparing text for transformer models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common email patterns to clean\n",
    "        self.email_patterns = {\n",
    "            'email_addresses': r'\\S+@\\S+\\.\\S+',\n",
    "            'urls': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "            'phone_numbers': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "            'excessive_whitespace': r'\\s+',\n",
    "            'special_chars': r'[^a-zA-Z0-9\\s.,!?;:]'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize email text\n",
    "        \n",
    "        This is crucial for good model performance!\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove email addresses (replace with token)\n",
    "        text = re.sub(self.email_patterns['email_addresses'], '[EMAIL]', text)\n",
    "        \n",
    "        # Remove URLs (replace with token)\n",
    "        text = re.sub(self.email_patterns['urls'], '[URL]', text)\n",
    "        \n",
    "        # Remove phone numbers (replace with token)\n",
    "        text = re.sub(self.email_patterns['phone_numbers'], '[PHONE]', text)\n",
    "        \n",
    "        # Clean excessive whitespace\n",
    "        text = re.sub(self.email_patterns['excessive_whitespace'], ' ', text)\n",
    "        \n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        \n",
    "        # Strip whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create additional features that help with classification\n",
    "        \n",
    "        This teaches us feature engineering for NLP\n",
    "        \"\"\"\n",
    "        print(\"🔧 Creating additional features...\")\n",
    "        \n",
    "        # Clean the text\n",
    "        df['clean_subject'] = df['subject'].apply(self.clean_text)\n",
    "        df['clean_body'] = df['body'].apply(self.clean_text)\n",
    "        \n",
    "        # Combine subject and body (important for email classification)\n",
    "        df['combined_text'] = df['clean_subject'] + ' ' + df['clean_body']\n",
    "        \n",
    "        # Feature engineering - these help models understand email patterns\n",
    "        df['has_urgent_words'] = df['combined_text'].str.contains(\n",
    "            r'urgent|emergency|critical|asap|immediate', case=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        df['has_question_marks'] = df['combined_text'].str.count('\\?')\n",
    "        df['has_exclamation'] = df['combined_text'].str.count('!')\n",
    "        df['has_caps'] = df['subject'].str.isupper().astype(int)\n",
    "        \n",
    "        # Count spam indicators\n",
    "        spam_words = ['free', 'win', 'prize', 'click', 'offer', 'limited time']\n",
    "        df['spam_word_count'] = df['combined_text'].apply(\n",
    "            lambda x: sum(1 for word in spam_words if word in x.lower())\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Feature engineering complete!\")\n",
    "        print(f\"   Created features: has_urgent_words, has_question_marks, has_exclamation, has_caps, spam_word_count\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_for_training(self, df: pd.DataFrame, test_size: float = 0.2) -> Tuple:\n",
    "        \"\"\"\n",
    "        Prepare the dataset for training\n",
    "        \n",
    "        This teaches us proper train/test splitting and data preparation\n",
    "        \"\"\"\n",
    "        print(f\"🎯 Preparing dataset for training...\")\n",
    "        \n",
    "        # Create features\n",
    "        df = self.create_features(df)\n",
    "        \n",
    "        # Features for training (we'll use the combined text)\n",
    "        X = df['combined_text'].values\n",
    "        y = df['label'].values\n",
    "        \n",
    "        # Split the dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Dataset split complete!\")\n",
    "        print(f\"   Training samples: {len(X_train)}\")\n",
    "        print(f\"   Testing samples: {len(X_test)}\")\n",
    "        print(f\"   Classes: {len(np.unique(y))}\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "        test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "        \n",
    "        print(f\"\\n   Training distribution:\")\n",
    "        for label, count in train_dist.items():\n",
    "            category = list(generator.categories.keys())[list(generator.categories.values()).index(label)]\n",
    "            print(f\"      {category}: {count} samples\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, df\n",
    "\n",
    "# Preprocess our data\n",
    "preprocessor = EmailTextPreprocessor()\n",
    "X_train, X_test, y_train, y_test, processed_df = preprocessor.prepare_for_training(email_df)\n",
    "\n",
    "# Show sample processed data\n",
    "print(\"\\n📧 Sample Processed Emails:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(3):\n",
    "    original_text = email_df.iloc[i]['subject'] + ' ' + email_df.iloc[i]['body']\n",
    "    processed_text = processed_df.iloc[i]['combined_text']\n",
    "    category = processed_df.iloc[i]['category']\n",
    "    \n",
    "    print(f\"\\n🏷️  {category.upper()}:\")\n",
    "    print(f\"   Original: {original_text[:100]}...\")\n",
    "    print(f\"   Processed: {processed_text[:100]}...\")\n",
    "    print(f\"   Features: urgent={processed_df.iloc[i]['has_urgent_words']}, \"\n",
    "          f\"caps={processed_df.iloc[i]['has_caps']}, \"\n",
    "          f\"spam_words={processed_df.iloc[i]['spam_word_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Part 3: LoRA Email Classifier Architecture\n",
    "\n",
    "### 🧠 Learning Objective:\n",
    "Now we'll build our **complete email classifier** using LoRA. This teaches us:\n",
    "\n",
    "- **Architecture design**: How to structure ML systems\n",
    "- **LoRA integration**: Practical application of our previous learning\n",
    "- **Custom datasets**: How to create PyTorch datasets\n",
    "- **End-to-end pipeline**: From raw text to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our LoRA implementation (we'll use the advanced version from Step 3)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# We'll recreate the necessary LoRA classes here for this notebook\n",
    "class AdvancedLoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Our proven LoRA implementation from Step 3\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 32.0,\n",
    "        dropout: float = 0.1,\n",
    "        init_lora_weights: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Get dimensions\n",
    "        self.in_features = original_layer.in_features\n",
    "        self.out_features = original_layer.out_features\n",
    "        \n",
    "        # LoRA parameters\n",
    "        self.lora_A = nn.Parameter(torch.empty(rank, self.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.empty(self.out_features, rank))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize weights\n",
    "        if init_lora_weights:\n",
    "            self.reset_lora_parameters()\n",
    "        \n",
    "        # Freeze original parameters\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def reset_lora_parameters(self):\n",
    "        import math\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.original_layer(x)\n",
    "        \n",
    "        # LoRA forward pass\n",
    "        lora_output = F.linear(x, self.lora_A)\n",
    "        lora_output = self.dropout(lora_output)\n",
    "        lora_output = F.linear(lora_output, self.lora_B.T)\n",
    "        \n",
    "        result += lora_output * self.scaling\n",
    "        return result\n",
    "    \n",
    "    def get_lora_parameters(self):\n",
    "        return [self.lora_A, self.lora_B]\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for email classification\n",
    "    \n",
    "    This teaches us how to create custom datasets for specific tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class LoRAEmailClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete email classifier with LoRA fine-tuning\n",
    "    \n",
    "    This is our main model that combines everything we've learned\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = 'distilbert-base-uncased',\n",
    "        num_classes: int = 5,\n",
    "        lora_rank: int = 8,\n",
    "        lora_alpha: float = 16.0,\n",
    "        lora_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"🏗️  Building LoRA Email Classifier...\")\n",
    "        print(f\"   Base model: {model_name}\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "        print(f\"   LoRA rank: {lora_rank}, alpha: {lora_alpha}\")\n",
    "        \n",
    "        # Load base model\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Apply LoRA to attention layers\n",
    "        self.lora_layers = {}\n",
    "        self._apply_lora(lora_rank, lora_alpha, lora_dropout)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.backbone.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize classifier weights\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        \n",
    "        self._print_parameter_stats()\n",
    "    \n",
    "    def _apply_lora(self, rank: int, alpha: float, dropout: float):\n",
    "        \"\"\"\n",
    "        Apply LoRA to attention layers\n",
    "        \"\"\"\n",
    "        target_modules = ['query', 'key', 'value']  # Focus on attention\n",
    "        replaced_count = 0\n",
    "        \n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if any(target in name for target in target_modules) and isinstance(module, nn.Linear):\n",
    "                # Replace with LoRA version\n",
    "                lora_layer = AdvancedLoRALayer(\n",
    "                    module, rank=rank, alpha=alpha, dropout=dropout\n",
    "                )\n",
    "                \n",
    "                # Set the module in the model\n",
    "                parent_module = self.backbone\n",
    "                module_parts = name.split('.')\n",
    "                \n",
    "                for part in module_parts[:-1]:\n",
    "                    parent_module = getattr(parent_module, part)\n",
    "                \n",
    "                setattr(parent_module, module_parts[-1], lora_layer)\n",
    "                self.lora_layers[name] = lora_layer\n",
    "                replaced_count += 1\n",
    "        \n",
    "        print(f\"   ✅ Applied LoRA to {replaced_count} layers\")\n",
    "    \n",
    "    def _print_parameter_stats(self):\n",
    "        \"\"\"\n",
    "        Print parameter statistics\n",
    "        \"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\n📊 Model Statistics:\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "        print(f\"   Memory reduction: {total_params / trainable_params:.1f}x\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the classifier\n",
    "        \"\"\"\n",
    "        # Get backbone outputs\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        cls_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'loss': loss,\n",
    "            'predictions': torch.argmax(logits, dim=-1)\n",
    "        }\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"\n",
    "        Get all trainable parameters for the optimizer\n",
    "        \"\"\"\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "\n",
    "# Create our email classifier\n",
    "model = LoRAEmailClassifier(\n",
    "    model_name='distilbert-base-uncased',\n",
    "    num_classes=5,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16.0\n",
    ")\n",
    "\n",
    "print(\"\\n✅ LoRA Email Classifier built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Part 4: Training Pipeline Setup\n",
    "\n",
    "### 🧠 Learning Objective:\n",
    "Now we'll create a **professional training pipeline** that you'd use in real projects. This teaches us:\n",
    "\n",
    "- **Training loop design**: How to structure training for production\n",
    "- **Monitoring and logging**: Track progress and debug issues\n",
    "- **Validation strategy**: Ensure our model generalizes well\n",
    "- **Best practices**: Professional ML engineering techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "print(\"🔄 Creating training and validation datasets...\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmailDataset(\n",
    "    texts=X_train.tolist(),\n",
    "    labels=y_train.tolist(),\n",
    "    tokenizer=model.tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = EmailDataset(\n",
    "    texts=X_test.tolist(),\n",
    "    labels=y_test.tolist(),\n",
    "    tokenizer=model.tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,  # Small batch size for memory efficiency\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,  # Larger batch for evaluation\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✅ Datasets created:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Testing batches: {len(test_loader)}\")\n",
    "print(f\"   Training batch size: 16\")\n",
    "print(f\"   Testing batch size: 32\")\n",
    "\n",
    "# Test a batch to make sure everything works\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\n🧪 Sample batch shapes:\")\n",
    "print(f\"   Input IDs: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"   Attention mask: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"   Labels: {sample_batch['labels'].shape}\")\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=sample_batch['input_ids'],\n",
    "        attention_mask=sample_batch['attention_mask'],\n",
    "        labels=sample_batch['labels']\n",
    "    )\n",
    "\n",
    "print(f\"\\n✅ Forward pass test successful:\")\n",
    "print(f\"   Output logits shape: {outputs['logits'].shape}\")\n",
    "print(f\"   Loss: {outputs['loss'].item():.4f}\")\n",
    "print(f\"   Predictions shape: {outputs['predictions'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}