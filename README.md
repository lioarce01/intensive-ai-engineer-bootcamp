# Itinerario Intensivo de 6 Meses para Convertirse en AI Engineer Especializado en LLMs

**Objetivo:** Prepararte como AI Engineer especializado en LLMs, agentes inteligentes y entrenamiento de modelos, listo para trabajar en empresas grandes.

**DuraciÃ³n:** 6 meses (8 horas semanales los fines de semana).

**Formato:** Estudio teÃ³rico + ejercicios prÃ¡cticos desde el inicio, con proyectos aplicados por mÃ³dulo y cierre con preparaciÃ³n profesional.

---

## Mes 1: Fundamentos matemÃ¡ticos + NLP moderno

### Semana 1-2: Repaso de Ã¡lgebra, cÃ¡lculo y probabilidad (solo lo esencial)

* **Temas:** Vectores, matrices, derivadas parciales, gradiente, probabilidad condicional, entropÃ­a
* **PrÃ¡ctica:** Implementar desde cero softmax, cross-entropy, funciones de activaciÃ³n
* **Recursos:**

  * [Essence of Linear Algebra (3Blue1Brown)](https://www.youtube.com/watch?v=fNk_zzaMoSs)
  * [Stanford CS229 notes](https://cs229.stanford.edu/)

### Semana 3-4: NLP clÃ¡sico y base de embeddings

* **Temas:** TokenizaciÃ³n, TF-IDF, word2vec, GloVe, FastText
* **PrÃ¡ctica:** Explorar y visualizar embeddings (TSNE), entrenar word2vec en corpus propio
* **Proyecto:** Explorador de similitud semÃ¡ntica entre textos cortos
* **Recursos:**

  * [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)
  * [Hugging Face Datasets](https://huggingface.co/docs/datasets)

---

## Mes 2: Transformers y LLMs

### Semana 5-6: Arquitectura Transformer en profundidad

* **Temas:** Multi-head attention, Positional Encoding, Encoder vs Decoder
* **PrÃ¡ctica:** ImplementaciÃ³n simple de Transformer encoder desde cero
* **Recursos:**

  * [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
  * [Jay Alammar Visual Guides](https://jalammar.github.io/)

### Semana 7-8: LLMs y fine-tuning bÃ¡sico

* **Temas:** Pretraining, fine-tuning, LoRA, QLoRA, PEFT
* **PrÃ¡ctica:** Fine-tuning de LLaMA 2 o Mistral usando Hugging Face + PEFT
* **Proyecto:** Clasificador de texto fine-tuneado para tareas de negocio
* **Recursos:**

  * [Hugging Face PEFT](https://github.com/huggingface/peft)
  * [LoRA Explained](https://lightning.ai/pages/community/tutorial/lora-from-scratch/)

---

## Mes 3: Agentes y RAG

### Semana 9-10: Fundamentos de agentes y herramientas

* **Temas:** LangChain, OpenAI Tools, planificadores, memoria, herramientas custom
* **PrÃ¡ctica:** Crear agentes multi-step con funciones custom (Python)
* **Proyecto:** Asistente automatizado que usa una API externa para responder
* **Recursos:**

  * [LangChain Docs](https://docs.langchain.com/)
  * [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)

### Semana 11-12: Retrieval-Augmented Generation (RAG)

* **Temas:** Chunking, embeddings, vectordb (FAISS, Chroma), pipelines RAG
* **PrÃ¡ctica:** Indexar documentos y crear RAG sobre base local
* **Proyecto:** RAG para documentaciÃ³n tÃ©cnica de empresa ficticia
* **Recursos:**

  * [HuggingFace RAG Tutorial](https://huggingface.co/blog/rag)
  * [LlamaIndex](https://docs.llamaindex.ai/)

---

## Mes 4: Deep Learning avanzado y entrenamiento de modelos

### Semana 13-14: Entrenamiento desde cero

* **Temas:** CÃ³digo desde cero para backprop, optimizadores (Adam, SGD), regularizaciÃ³n
* **PrÃ¡ctica:** Entrenar MLP sobre texto/tablas
* **Proyecto:** Clasificador desde cero sin usar frameworks
* **Recursos:**

  * [CS231n (Stanford)](https://cs231n.github.io/)

### Semana 15-16: Entrenamiento de LLMs + evaluaciÃ³n

* **Temas:** Dataset curation, tokenizaciÃ³n, data collators, evaluaciÃ³n (BLEU, ROUGE, perplexity)
* **PrÃ¡ctica:** Entrenar pequeÃ±o modelo autoregresivo con Transformers
* **Proyecto:** Mini GPT entrenado con corpus propio

---

## Mes 5: Despliegue e infraestructura

### Semana 17-18: Infraestructura de inferencia

* **Temas:** vLLM, TGI (Text Generation Inference), optimizaciÃ³n en inferencia
* **PrÃ¡ctica:** Servir un modelo usando vLLM con interfaz REST
* **Recursos:**

  * [vLLM GitHub](https://github.com/vllm-project/vllm)
  * [TGI GitHub](https://github.com/huggingface/text-generation-inference)

### Semana 19-20: MLOps aplicado a LLMs

* **Temas:** Tracking (MLflow), versionado, Docker, deployment con FastAPI
* **Proyecto:** Microservicio AI desplegado con FastAPI + Docker
* **Recursos:**

  * [Full Stack LLM Course (Free)](https://fullstackdeeplearning.com/llm-bootcamp/)

---

## Mes 6: Portafolio y entrevistas

### Semana 21: Armado de portafolio

* **Tareas:**

  * Repos GitHub ordenados y con README explicativos
  * Publicar proyectos en Hugging Face o Gradio
  * Crear blog tÃ©cnico (Medium, Hashnode) explicando un proyecto

### Semana 22-23: PreparaciÃ³n para entrevistas tÃ©cnicas

* **Temas:**

  * Preguntas comunes de AI Engineer (arquitecturas, tradeoffs, escalabilidad)
  * Problemas en ML (overfitting, drift, bias, evaluaciÃ³n)
  * Repaso de conceptos clave (attention, embeddings, RLHF, etc)
* **PrÃ¡ctica:** Mock interviews y repaso de notebooks

### Semana 24: Aplicaciones y roadmap futuro

* **Tareas:**

  * Aplicar a posiciones con buen ajuste
  * Identificar posibles contribuciones open source
  * Definir Ã¡rea de especializaciÃ³n (e.g. agentes, RAG, MLOps)

---

## Mes 7 (Opcional): ExtensiÃ³n hacia AGI

### Semana 25-26: Arquitecturas cognitivas y neuro-symbolic AI

* **Temas:** SOAR, ACT-R, OpenCog, Leabra
* **Lectura recomendada:**

  * DeepMind Research Blog
  * *How to Create a Mind* â€“ Ray Kurzweil
* **PrÃ¡ctica:** DiseÃ±ar agente hÃ­brido simbÃ³lico + LLM

### Semana 27: Meta-Learning y Continual Learning

* **Temas:** MAML, Reptile, EWC, aprendizaje incremental
* **Proyecto:** Entrenar agente con aprendizaje continuo sin olvidar tareas

### Semana 28: Reinforcement Learning y RLHF

* **Temas:** OpenAI Gym, PPO, RLHF pipeline
* **PrÃ¡ctica:** Fine-tuning con feedback humano simulado

### Semana 29: Autoconciencia, reflexiÃ³n, cadenas de pensamiento

* **Temas:** Self-ask, self-verification, auto-crÃ­tica de respuestas
* **PrÃ¡ctica:** LLM que evalÃºa y corrige su propio output

### Semana 30: Alineamiento y Ã©tica de AGI

* **Temas:** Value alignment, interpretabilidad, riesgos, sesgos
* **Lecturas clave:**

  * *The Alignment Problem* â€“ Brian Christian
  * *Superintelligence* â€“ Nick Bostrom

---

> ğŸ” Consejo final: Mantente actualizado con papers, conferencias (NeurIPS, ICLR, ICML), y participa en comunidades (Discord, Hugging Face, Twitter). Documentar tu aprendizaje te harÃ¡ destacar.
