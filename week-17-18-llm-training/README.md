# Week 17-18: Entrenamiento de LLMs desde Cero

> **Objetivo**: Entrenar un modelo de lenguaje desde cero, comprendiendo dataset curation, mÃ©tricas de evaluaciÃ³n y tÃ©cnicas de entrenamiento eficiente.

## ğŸ¯ Objetivos de Aprendizaje

1. **Dataset Curation**: Aprender a recolectar, limpiar y preparar datos de calidad para entrenar LLMs
2. **MÃ©tricas de EvaluaciÃ³n**: Dominar perplexity, BLEU, ROUGE y mÃ©tricas modernas
3. **Efficient Training**: Implementar tÃ©cnicas de entrenamiento eficiente (gradient accumulation, mixed precision, etc.)
4. **Training from Scratch**: Entrenar un mini Language Model funcional

## ğŸ“š Estructura del MÃ³dulo

```
week-17-18-llm-training/
â”œâ”€â”€ dataset-curation/          # Scripts para curaciÃ³n de datasets
â”‚   â”œâ”€â”€ data_collection.py     # Web scraping y recolecciÃ³n
â”‚   â”œâ”€â”€ data_cleaning.py       # Limpieza y preprocesamiento
â”‚   â”œâ”€â”€ tokenization.py        # TokenizaciÃ³n y preparaciÃ³n
â”‚   â””â”€â”€ quality_filters.py     # Filtros de calidad
â”œâ”€â”€ training-metrics/          # ImplementaciÃ³n de mÃ©tricas
â”‚   â”œâ”€â”€ perplexity.py         # CÃ¡lculo de perplexity
â”‚   â”œâ”€â”€ bleu_rouge.py         # BLEU y ROUGE scores
â”‚   â”œâ”€â”€ custom_metrics.py     # MÃ©tricas personalizadas
â”‚   â””â”€â”€ evaluation.py         # Pipeline de evaluaciÃ³n
â”œâ”€â”€ efficient-training/        # TÃ©cnicas de entrenamiento eficiente
â”‚   â”œâ”€â”€ gradient_accumulation.py
â”‚   â”œâ”€â”€ mixed_precision.py
â”‚   â”œâ”€â”€ gradient_checkpointing.py
â”‚   â””â”€â”€ distributed_training.py
â”œâ”€â”€ mini-lm-project/          # Proyecto principal
â”‚   â”œâ”€â”€ model.py              # Arquitectura del modelo
â”‚   â”œâ”€â”€ train.py              # Script de entrenamiento
â”‚   â”œâ”€â”€ config.py             # ConfiguraciÃ³n
â”‚   â””â”€â”€ inference.py          # Inferencia
â””â”€â”€ notebooks/                # Notebooks educativos
    â”œâ”€â”€ 01_dataset_exploration.ipynb
    â”œâ”€â”€ 02_metrics_deep_dive.ipynb
    â””â”€â”€ 03_training_mini_lm.ipynb
```

## ğŸš€ Proyecto Principal: Mini Language Model

### Especificaciones del Modelo
- **Arquitectura**: Transformer decoder-only (estilo GPT)
- **TamaÃ±o**: ~50M parÃ¡metros
- **Dataset**: Subset de Wikipedia + cÃ³digo pÃºblico
- **Training Time**: ~2-4 horas en GPU (T4/V100)
- **Target Perplexity**: <30 en validation set

### Componentes Clave

#### 1. Dataset Curation
```python
# Ejemplo de pipeline de curaciÃ³n
pipeline = DatasetPipeline([
    WebScraper(sources=['wikipedia', 'github']),
    QualityFilter(min_length=100, max_length=1024),
    DeduplicationFilter(),
    TokenCounter(target_tokens=100_000_000),
    Tokenizer(vocab_size=32_000)
])
```

#### 2. MÃ©tricas de EvaluaciÃ³n
- **Perplexity**: Medida de incertidumbre del modelo
- **BLEU/ROUGE**: Para generaciÃ³n de texto
- **Custom Metrics**: Coherencia, diversidad, toxicidad

#### 3. Entrenamiento Eficiente
- Gradient Accumulation (simular batch size grande)
- Mixed Precision Training (FP16)
- Gradient Checkpointing (ahorrar memoria)
- Distributed Data Parallel (mÃºltiples GPUs)

## ğŸ“‹ Tasks del Proyecto

### Task 1: Dataset Curation (DÃ­as 1-3)
- [ ] Recolectar 100M tokens de texto de calidad
- [ ] Implementar filtros de calidad
- [ ] Crear pipeline de preprocesamiento
- [ ] Validar distribuciÃ³n de datos

### Task 2: Metrics Implementation (DÃ­as 4-5)
- [ ] Implementar perplexity desde cero
- [ ] Integrar BLEU y ROUGE
- [ ] Crear dashboard de mÃ©tricas
- [ ] Validar contra implementaciones estÃ¡ndar

### Task 3: Training Setup (DÃ­as 6-8)
- [ ] Definir arquitectura del modelo
- [ ] Implementar efficient training techniques
- [ ] Configurar experiment tracking (MLflow/W&B)
- [ ] Setup validation pipeline

### Task 4: Model Training (DÃ­as 9-12)
- [ ] Entrenar modelo base
- [ ] Monitorear mÃ©tricas en tiempo real
- [ ] Ajustar hiperparÃ¡metros
- [ ] Validar convergencia

### Task 5: Evaluation & Deployment (DÃ­as 13-14)
- [ ] EvaluaciÃ³n rigurosa del modelo
- [ ] Comparar con baselines
- [ ] Documentar resultados
- [ ] Deploy API de inferencia

## ğŸ› ï¸ Setup

```bash
# Instalar dependencias
pip install -r requirements.txt

# Descargar dataset de ejemplo
python dataset-curation/download_data.py

# Entrenar modelo
python mini-lm-project/train.py --config configs/mini_lm.yaml

# Evaluar modelo
python mini-lm-project/evaluate.py --checkpoint checkpoints/best_model.pt
```

## ğŸ“Š MÃ©tricas de Ã‰xito

| MÃ©trica | Target | Notas |
|---------|--------|-------|
| **Perplexity (val)** | <30 | Medida principal de calidad |
| **Training Time** | <4 hours | En GPU T4/V100 |
| **Model Size** | ~50M params | Balancear tamaÃ±o vs performance |
| **Throughput** | >100 tokens/s | Velocidad de generaciÃ³n |
| **BLEU Score** | >20 | En task de generaciÃ³n |

## ğŸ” Conceptos Clave

### Perplexity
```python
# Perplexity = exp(cross_entropy_loss)
perplexity = torch.exp(loss)
```
- **InterpretaciÃ³n**: CuÃ¡ntas opciones "promedio" el modelo considera por token
- **Bueno**: <20 (excelente), 20-50 (aceptable), >50 (necesita mejoras)

### Dataset Quality
- **Diversidad**: MÃºltiples dominios y estilos
- **Limpieza**: Sin duplicados, errores o contenido tÃ³xico
- **Balance**: RepresentaciÃ³n equilibrada de temas
- **TamaÃ±o**: Suficiente para generalizaciÃ³n (100M+ tokens)

### Efficient Training
- **Gradient Accumulation**: Simular batch size de 1024 con GPU pequeÃ±a
- **Mixed Precision**: 2x speedup con AMP (Automatic Mixed Precision)
- **Gradient Checkpointing**: 40% menos memoria a cambio de 20% mÃ¡s tiempo

## ğŸ“š Recursos

### Papers Fundamentales
- [GPT-2 Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - Arquitectura y training
- [Scaling Laws](https://arxiv.org/abs/2001.08361) - RelaciÃ³n entre tamaÃ±o y performance
- [Chinchilla Paper](https://arxiv.org/abs/2203.15556) - Optimal training compute

### Implementaciones de Referencia
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Minimal GPT implementation
- [TinyLlama](https://github.com/jzhang38/TinyLlama) - 1.1B parameter model training
- [minGPT](https://github.com/karpathy/minGPT) - Educational GPT

### Herramientas
- **Datasets**: Hugging Face Datasets, Common Crawl
- **Training**: PyTorch, DeepSpeed, Accelerate
- **Monitoring**: Weights & Biases, TensorBoard, MLflow
- **Evaluation**: lm-evaluation-harness

## ğŸ“ Entregables

1. **Mini LM Funcional** (~50M params)
   - CÃ³digo de training reproducible
   - Checkpoints del modelo
   - MÃ©tricas de evaluaciÃ³n

2. **Dataset Curado** (100M tokens)
   - Pipeline de curaciÃ³n documentado
   - EstadÃ­sticas y visualizaciones
   - ValidaciÃ³n de calidad

3. **Reporte de Training**
   - Curvas de learning
   - AnÃ¡lisis de mÃ©tricas
   - ComparaciÃ³n con baselines
   - Lecciones aprendidas

4. **API de Inferencia**
   - Endpoint FastAPI
   - GeneraciÃ³n de texto
   - MÃ©tricas de latencia

## ğŸ’¡ Tips PrÃ¡cticos

1. **Start Small**: Prueba con 10M tokens antes de escalar
2. **Monitor Always**: Usa W&B o TensorBoard desde el dÃ­a 1
3. **Validate Early**: Revisa outputs cada 1000 steps
4. **Save Frequently**: Checkpoints cada hora durante training
5. **Document Everything**: El training es caro, documenta los experimentos

## ğŸš§ Troubleshooting ComÃºn

### Perplexity no baja
- Verificar learning rate (tÃ­picamente 3e-4 para Adam)
- Revisar calidad de datos
- Aumentar model capacity
- Entrenar por mÃ¡s tiempo

### Out of Memory
- Reducir batch size
- Activar gradient checkpointing
- Usar gradient accumulation
- Reducir sequence length

### Training inestable
- Usar gradient clipping (max_norm=1.0)
- Reducir learning rate
- Aumentar warmup steps
- Revisar data preprocessing

## ğŸ¯ Siguientes Pasos

DespuÃ©s de completar este mÃ³dulo:
- **Week 19-20**: Multimodal AI y Responsible AI
- **Advanced**: Implementar RLHF para alineamiento
- **Production**: Deployment en Hugging Face Spaces
- **Research**: Contribuir a proyectos open source de LLMs

---

**Tiempo estimado**: 14 dÃ­as
**Dificultad**: â­â­â­â­â­ (Avanzado)
**Prerequisitos**: Weeks 5-8 (Transformers y Fine-tuning)
